{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# gensim doc2vec & IMDB sentiment dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "TODO: section on introduction & motivation\n",
    "\n",
    "TODO: prerequisites + dependencies (statsmodels, patsy, ?)\n",
    "\n",
    "### Requirements\n",
    "Following are the dependencies for this tutorial:\n",
    "    - testfixtures\n",
    "    - statsmodels\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The data is small enough to be read into memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75000 train docs, 25000 test docs\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from collections import namedtuple\n",
    "\n",
    "SentimentDocument = namedtuple('SentimentDocument', 'words tags sentiment')\n",
    "\n",
    "train_docs, test_docs = [], []  # will hold all docs in original order\n",
    "for name, docs, sentiment in [\n",
    "    ('train_pos', train_docs, 1.0), ('train_neg', train_docs, 0.0), ('train_unsup', train_docs, None),\n",
    "    ('test_pos', test_docs, 1.0), ('test_neg', test_docs, 0.0),\n",
    "]:\n",
    "    with open(f'../data/imdb_sentiment/{name}.txt', encoding='utf-8') as sr:\n",
    "        for i, line in enumerate(sr):\n",
    "            docs.append(SentimentDocument(line.split(), [f'{name}_{i}'], sentiment))\n",
    "\n",
    "print(f'{len(train_docs)} train docs, {len(test_docs)} test docs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Set-up Doc2Vec Training & Evaluation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Approximating experiment of Le & Mikolov [\"Distributed Representations of Sentences and Documents\"](http://cs.stanford.edu/~quocle/paragraph_vector.pdf), also with guidance from Mikolov's [example go.sh](https://groups.google.com/d/msg/word2vec-toolkit/Q49FIrNOQRo/J6KG8mUj45sJ):\n",
    "\n",
    "`./word2vec -train ../alldata-id.txt -output vectors.txt -cbow 0 -size 100 -window 10 -negative 5 -hs 0 -sample 1e-4 -threads 40 -binary 0 -iter 20 -min-count 1 -sentence-vectors 1`\n",
    "\n",
    "Parameter choices below vary:\n",
    "\n",
    "* 100-dimensional vectors, as the 400d vectors of the paper don't seem to offer much benefit on this task\n",
    "* similarly, frequent word subsampling seems to decrease sentiment-prediction accuracy, so it's left out\n",
    "* `cbow=0` means skip-gram which is equivalent to the paper's 'PV-DBOW' mode, matched in gensim with `dm=0`\n",
    "* added to that DBOW model are two DM models, one which averages context vectors (`dm_mean`) and one which concatenates them (`dm_concat`, resulting in a much larger, slower, more data-hungry model)\n",
    "* a `min_count=2` saves quite a bit of model memory, discarding only words that appear in a single doc (and are thus no more expressive than the unique-to-each doc vectors themselves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8)\n",
      "Doc2Vec(dbow,d100,hs,mc2,s0.001,t8)\n",
      "Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "from collections import OrderedDict\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"this will be painfully slow otherwise\"\n",
    "\n",
    "simple_models = [\n",
    "    # PV-DM w/concatenation - window=5 (both sides) approximates paper's 10-word total window size\n",
    "    Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=0, hs=1, min_count=2, workers=cores),\n",
    "    # PV-DBOW \n",
    "    Doc2Vec(dm=0, size=100, negative=0, hs=1, min_count=2, workers=cores),\n",
    "    # PV-DM w/average\n",
    "    Doc2Vec(dm=1, dm_mean=1, size=100, window=10, negative=0, hs=1, min_count=2, workers=cores),\n",
    "]\n",
    "\n",
    "for model in simple_models:\n",
    "    model.build_vocab(train_docs)\n",
    "    print(model)\n",
    "\n",
    "models_by_name = OrderedDict((str(model), model) for model in simple_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Following the paper, we also evaluate models in pairs. These wrappers return the concatenation of the vectors from each model. (Only the singular models are trained.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "models_by_name['dbow+dmm'] = ConcatenatedDoc2Vec([simple_models[1], simple_models[2]])\n",
    "models_by_name['dbow+dmc'] = ConcatenatedDoc2Vec([simple_models[1], simple_models[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Predictive Evaluation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Helper methods for evaluating error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from random import sample\n",
    "\n",
    "# for timing\n",
    "from contextlib import contextmanager\n",
    "from timeit import default_timer\n",
    "import time \n",
    "\n",
    "@contextmanager\n",
    "def elapsed_timer():\n",
    "    start = default_timer()\n",
    "    elapser = lambda: default_timer() - start\n",
    "    yield lambda: elapser()\n",
    "    end = default_timer()\n",
    "    elapser = lambda: end-start\n",
    "    \n",
    "def logistic_predictor_from_data(train_targets, train_regressors):\n",
    "    logit = sm.Logit(train_targets, train_regressors)\n",
    "    predictor = logit.fit(disp=0)\n",
    "    #print(predictor.summary())\n",
    "    return predictor\n",
    "\n",
    "def error_rate_for_model(test_model, train_set, test_set, infer_steps=3, infer_alpha=0.1, infer_subsample=0.1):\n",
    "    \"\"\"Report error rate on test_doc sentiments, using supplied model and train_docs\"\"\"\n",
    "\n",
    "    train_targets, train_regressors = zip(*[\n",
    "        (doc.sentiment, test_model.docvecs[doc.tags[0]])\n",
    "        for doc in train_set if doc.sentiment is not None\n",
    "    ])\n",
    "    train_regressors = sm.add_constant(train_regressors)\n",
    "    predictor = logistic_predictor_from_data(train_targets, train_regressors)\n",
    "\n",
    "    test_data = test_set\n",
    "    if infer_subsample < 1.0:\n",
    "        test_data = sample(test_data, int(infer_subsample * len(test_data)))\n",
    "    test_regressors = [test_model.infer_vector(doc.words, steps=infer_steps, alpha=infer_alpha) for doc in test_data]\n",
    "    test_regressors = sm.add_constant(test_regressors)\n",
    "    \n",
    "    # predict & evaluate\n",
    "    test_predictions = predictor.predict(test_regressors)\n",
    "    corrects = sum(np.rint(test_predictions) == [doc.sentiment for doc in test_data])\n",
    "    errors = len(test_predictions) - corrects\n",
    "    error_rate = float(errors) / len(test_predictions)\n",
    "    return (error_rate, errors, len(test_predictions), predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Bulk Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Using explicit multiple-pass, alpha-reduction approach as sketched in [gensim doc2vec blog post](http://radimrehurek.com/2014/12/doc2vec-tutorial/) â€“ with added shuffling of corpus on each pass.\n",
    "\n",
    "Note that vector training is occurring on *all* documents of the dataset, which includes all TRAIN/TEST/DEV docs.\n",
    "\n",
    "Evaluation of each model's sentiment-predictive power is repeated after each pass, as an error rate (lower is better), to see the rates-of-relative-improvement. The base numbers reuse the TRAIN and TEST vectors stored in the models for the logistic regression, while the _inferred_ results use newly-inferred TEST vectors. \n",
    "\n",
    "(On a 4-core 2.6Ghz Intel Core i7, these 20 passes training and evaluating 3 main models takes about an hour.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "best_error = defaultdict(lambda :1.0)  # to selectively-print only best errors achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START 2017-04-25 19:17:23.675307\n",
      "*0.286000 : 1 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8)_inferred 32.2s 7.8s\n",
      "*0.168000 : 1 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8)_inferred 10.5s 2.4s\n",
      "*0.216800 : 1 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8)_inferred 14.9s 3.2s\n",
      "*0.182400 : 1 passes : dbow+dmm_inferred 0.0s 6.1s\n",
      "*0.194400 : 1 passes : dbow+dmc_inferred 0.0s 11.4s\n",
      "completed pass 1 at alpha 0.025000\n",
      "*0.256800 : 2 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8)_inferred 34.2s 7.6s\n",
      "*0.146000 : 2 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8)_inferred 12.1s 3.0s\n",
      " 0.244000 : 2 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8)_inferred 17.1s 3.4s\n",
      "*0.148800 : 2 passes : dbow+dmm_inferred 0.0s 6.6s\n",
      "*0.154800 : 2 passes : dbow+dmc_inferred 0.0s 11.5s\n",
      "completed pass 2 at alpha 0.023800\n",
      "*0.212400 : 3 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8)_inferred 36.0s 8.0s\n",
      "*0.130000 : 3 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8)_inferred 11.7s 2.5s\n",
      " 0.218800 : 3 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8)_inferred 16.7s 3.3s\n",
      "*0.147200 : 3 passes : dbow+dmm_inferred 0.0s 6.7s\n",
      "*0.132400 : 3 passes : dbow+dmc_inferred 0.0s 11.3s\n",
      "completed pass 3 at alpha 0.022600\n",
      "*0.205600 : 4 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8)_inferred 36.0s 8.1s\n",
      "*0.129600 : 4 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8)_inferred 11.5s 2.6s\n",
      "*0.210400 : 4 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8)_inferred 16.4s 3.2s\n",
      "*0.133600 : 4 passes : dbow+dmm_inferred 0.0s 6.8s\n",
      " 0.132800 : 4 passes : dbow+dmc_inferred 0.0s 11.6s\n",
      "completed pass 4 at alpha 0.021400\n",
      "*0.202000 : 5 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8)_inferred 32.3s 7.3s\n",
      "*0.123200 : 5 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8)_inferred 9.9s 2.3s\n",
      "*0.205200 : 5 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8)_inferred 14.6s 3.2s\n",
      "*0.120800 : 5 passes : dbow+dmm_inferred 0.0s 6.0s\n",
      " 0.139200 : 5 passes : dbow+dmc_inferred 0.0s 10.5s\n",
      "completed pass 5 at alpha 0.020200\n",
      "*0.198000 : 6 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8)_inferred 30.7s 7.3s\n",
      "*0.117600 : 6 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8)_inferred 9.9s 2.3s\n",
      " 0.212800 : 6 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8)_inferred 15.3s 3.2s\n",
      " 0.128400 : 6 passes : dbow+dmm_inferred 0.0s 6.0s\n",
      " 0.136400 : 6 passes : dbow+dmc_inferred 0.0s 10.4s\n",
      "completed pass 6 at alpha 0.019000\n",
      " 0.199200 : 7 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8)_inferred 30.7s 7.7s\n",
      "*0.116800 : 7 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8)_inferred 10.3s 2.4s\n",
      "*0.204400 : 7 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8)_inferred 17.1s 3.2s\n",
      " 0.134400 : 7 passes : dbow+dmm_inferred 0.0s 6.2s\n",
      " 0.150400 : 7 passes : dbow+dmc_inferred 0.0s 10.9s\n",
      "completed pass 7 at alpha 0.017800\n",
      " 0.204400 : 8 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8)_inferred 33.8s 7.7s\n",
      " 0.122800 : 8 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8)_inferred 10.6s 2.4s\n",
      "*0.192400 : 8 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8)_inferred 15.2s 3.3s\n",
      " 0.132000 : 8 passes : dbow+dmm_inferred 0.0s 6.3s\n",
      "*0.130000 : 8 passes : dbow+dmc_inferred 0.0s 10.7s\n",
      "completed pass 8 at alpha 0.016600\n",
      " 0.203200 : 9 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8)_inferred 32.1s 7.8s\n",
      " 0.128000 : 9 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8)_inferred 10.7s 2.9s\n",
      " 0.208800 : 9 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8)_inferred 16.7s 3.5s\n",
      "*0.119600 : 9 passes : dbow+dmm_inferred 0.0s 6.9s\n",
      "*0.120000 : 9 passes : dbow+dmc_inferred 0.0s 11.6s\n",
      "completed pass 9 at alpha 0.015400\n",
      " 0.200800 : 10 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8)_inferred 36.8s 8.5s\n",
      " 0.122000 : 10 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8)_inferred 13.1s 2.5s\n",
      " 0.196800 : 10 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8)_inferred 18.2s 3.5s\n",
      "*0.119200 : 10 passes : dbow+dmm_inferred 0.0s 6.8s\n",
      " 0.121600 : 10 passes : dbow+dmc_inferred 0.0s 11.9s\n",
      "completed pass 10 at alpha 0.014200\n",
      " 0.202000 : 11 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8)_inferred 39.5s 8.5s\n",
      " 0.129600 : 11 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8)_inferred 12.9s 2.6s\n",
      "*0.190000 : 11 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8)_inferred 17.7s 3.5s\n",
      " 0.133600 : 11 passes : dbow+dmm_inferred 0.0s 6.9s\n",
      " 0.136800 : 11 passes : dbow+dmc_inferred 0.0s 11.5s\n",
      "completed pass 11 at alpha 0.013000\n",
      " 0.198000 : 12 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8)_inferred 35.1s 8.2s\n",
      " 0.127600 : 12 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8)_inferred 12.0s 2.6s\n",
      " 0.191200 : 12 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8)_inferred 17.0s 3.3s\n",
      " 0.135200 : 12 passes : dbow+dmm_inferred 0.0s 6.7s\n",
      " 0.144800 : 12 passes : dbow+dmc_inferred 0.0s 11.6s\n",
      "completed pass 12 at alpha 0.011800\n",
      " 0.201600 : 13 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8)_inferred 35.1s 8.0s\n",
      " 0.130400 : 13 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8)_inferred 11.8s 2.5s\n",
      " 0.199600 : 13 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8)_inferred 17.0s 3.4s\n",
      " 0.137200 : 13 passes : dbow+dmm_inferred 0.0s 6.6s\n",
      " 0.124000 : 13 passes : dbow+dmc_inferred 0.0s 11.7s\n",
      "completed pass 13 at alpha 0.010600\n",
      " 0.210800 : 14 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8)_inferred 37.7s 7.9s\n",
      " 0.128800 : 14 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8)_inferred 10.7s 2.4s\n",
      "*0.184800 : 14 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8)_inferred 17.7s 3.7s\n",
      " 0.122400 : 14 passes : dbow+dmm_inferred 0.0s 6.4s\n",
      " 0.130400 : 14 passes : dbow+dmc_inferred 0.0s 11.5s\n",
      "completed pass 14 at alpha 0.009400\n",
      " 0.209200 : 15 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8)_inferred 38.7s 8.5s\n",
      " 0.125600 : 15 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8)_inferred 12.7s 2.6s\n",
      "*0.183600 : 15 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8)_inferred 17.3s 3.4s\n",
      " 0.128800 : 15 passes : dbow+dmm_inferred 0.0s 7.0s\n",
      " 0.133600 : 15 passes : dbow+dmc_inferred 0.0s 12.0s\n",
      "completed pass 15 at alpha 0.008200\n",
      " 0.206800 : 16 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8)_inferred 37.7s 8.2s\n",
      " 0.131200 : 16 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8)_inferred 12.9s 2.6s\n",
      " 0.188000 : 16 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8)_inferred 17.1s 3.9s\n",
      " 0.133600 : 16 passes : dbow+dmm_inferred 0.0s 6.4s\n",
      " 0.124400 : 16 passes : dbow+dmc_inferred 0.0s 11.0s\n",
      "completed pass 16 at alpha 0.007000\n",
      "*0.196800 : 17 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8)_inferred 35.2s 8.3s\n",
      " 0.120800 : 17 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8)_inferred 12.7s 2.6s\n",
      "*0.176400 : 17 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8)_inferred 18.0s 3.5s\n",
      " 0.133600 : 17 passes : dbow+dmm_inferred 0.0s 7.1s\n",
      " 0.127600 : 17 passes : dbow+dmc_inferred 0.0s 11.9s\n",
      "completed pass 17 at alpha 0.005800\n",
      " 0.200400 : 18 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8)_inferred 36.1s 8.0s\n",
      " 0.119200 : 18 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8)_inferred 12.2s 2.6s\n",
      " 0.185600 : 18 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8)_inferred 16.4s 3.2s\n",
      " 0.130000 : 18 passes : dbow+dmm_inferred 0.0s 6.0s\n",
      " 0.126000 : 18 passes : dbow+dmc_inferred 0.0s 11.9s\n",
      "completed pass 18 at alpha 0.004600\n",
      "*0.194400 : 19 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8)_inferred 37.9s 8.3s\n",
      " 0.129600 : 19 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8)_inferred 12.8s 2.6s\n",
      " 0.181600 : 19 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8)_inferred 18.0s 3.5s\n",
      " 0.134800 : 19 passes : dbow+dmm_inferred 0.0s 6.8s\n",
      " 0.126800 : 19 passes : dbow+dmc_inferred 0.0s 11.9s\n",
      "completed pass 19 at alpha 0.003400\n",
      " 0.202800 : 20 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8)_inferred 37.1s 8.2s\n",
      " 0.128400 : 20 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8)_inferred 12.8s 2.5s\n",
      " 0.179200 : 20 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8)_inferred 15.7s 3.3s\n",
      " 0.120400 : 20 passes : dbow+dmm_inferred 0.0s 6.9s\n",
      " 0.132000 : 20 passes : dbow+dmc_inferred 0.0s 11.4s\n",
      "completed pass 20 at alpha 0.002200\n",
      "END 2017-04-25 19:49:15.916447\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "import datetime\n",
    "\n",
    "alpha, min_alpha, passes = (0.025, 0.001, 20)\n",
    "alpha_delta = (alpha - min_alpha) / passes\n",
    "\n",
    "print(f'START {datetime.datetime.now()}')\n",
    "\n",
    "doc_list = list(train_docs)\n",
    "for epoch in range(passes):\n",
    "    shuffle(doc_list)  # shuffling gets best results\n",
    "    \n",
    "    for name, train_model in models_by_name.items():\n",
    "        # train\n",
    "        duration = 'na'\n",
    "        train_model.alpha, train_model.min_alpha = alpha, alpha\n",
    "        with elapsed_timer() as elapsed:\n",
    "            if train_model in simple_models:\n",
    "                train_model.train(doc_list, total_examples=model.corpus_count, epochs=1)\n",
    "            duration = f'{elapsed():.1f}'\n",
    "            \n",
    "        # evaluate\n",
    "        eval_duration = ''\n",
    "        with elapsed_timer() as eval_elapsed:\n",
    "            infer_err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs, test_docs)\n",
    "        eval_duration = f'{eval_elapsed():.1f}'\n",
    "        best_indicator = ' '\n",
    "        if infer_err < best_error[name + '_inferred']:\n",
    "            best_error[name + '_inferred'] = infer_err\n",
    "            best_indicator = '*'\n",
    "        print(f'{best_indicator}{infer_err:f} : {epoch + 1} passes : {name}_inferred {duration}s {eval_duration}s')\n",
    "\n",
    "    print(f'completed pass {epoch + 1} at alpha {alpha:f}')\n",
    "    alpha -= alpha_delta\n",
    "    \n",
    "print(f'END {datetime.datetime.now()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Achieved Sentiment-Prediction Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.116800 Doc2Vec(dbow,d100,hs,mc2,s0.001,t8)_inferred\n",
      "0.119200 dbow+dmm_inferred\n",
      "0.120000 dbow+dmc_inferred\n",
      "0.176400 Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8)_inferred\n",
      "0.194400 Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8)_inferred\n"
     ]
    }
   ],
   "source": [
    "# print best error rates achieved\n",
    "for rate, name in sorted((rate, name) for name, rate in best_error.items()):\n",
    "    print(f'{rate:f} {name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In my testing, unlike the paper's report, DBOW performs best. Concatenating vectors from different models only offers a small predictive improvement. The best results I've seen are still just under 10% error rate, still a ways from the paper's 7.42%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Examining Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Are inferred vectors close to the precalculated ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for doc 38355...\n",
      "Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8):\n",
      " [('train_unsup_13355', 0.9153209924697876), ('train_unsup_32658', 0.44021445512771606), ('train_neg_2435', 0.41127586364746094)]\n",
      "Doc2Vec(dbow,d100,hs,mc2,s0.001,t8):\n",
      " [('train_unsup_13355', 0.9604182839393616), ('train_pos_7769', 0.4150095582008362), ('train_unsup_39714', 0.4141600430011749)]\n",
      "Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8):\n",
      " [('train_unsup_13355', 0.8782677054405212), ('train_pos_225', 0.4277144968509674), ('train_neg_117', 0.4054872989654541)]\n"
     ]
    }
   ],
   "source": [
    "doc_id = np.random.randint(simple_models[0].docvecs.count)  # pick random doc; re-run cell for more examples\n",
    "print('for doc %d...' % doc_id)\n",
    "for model in simple_models:\n",
    "    inferred_docvec = model.infer_vector(train_docs[doc_id].words)\n",
    "    print('%s:\\n %s' % (model, model.docvecs.most_similar([inferred_docvec], topn=3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "(Yes, here the stored vector from 20 epochs of training is usually one of the closest to a freshly-inferred vector for the same words. Note the defaults for inference are very abbreviated â€“ just 3 steps starting at a high alpha â€“ and likely need tuning for other applications.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Do close documents seem more related than distant ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET (15622): Â«i couldn't give this film a bad rating or bad review for two reasons : robin williams and toni collete . the film has the potential of being a thriller and there are some slight disturbing elements that lean to the psychological which was something the film could have focused a little on . robin williams plays gabriel noon , a storytelling night time deejay who is going through personal issues : his lover moves out and gabriel is having what seems to be a case of storyteller's block . one day he receives and reads a story written by a dying 14-year old boy named pete boland ( rory culkin ) . pete tells the story of his life and the abuse he suffered at the hands of his parents . he lives with his adopted mother and social worker , donna boland ( toni collette ) . gabriel is fascinated and begins a friendship with pete , but things seem strange when gabriel attempts to meet him and discovers the possibility that pete boland may not even exist . i won't go into detail because i don't want to spoil the film , but i will tell you this : it is quite predictable . fascinating atmosphere for telling a story and good performances from robin williams and toni collette , who i thought was the film's key character . collette is without question one of the most talented and loveliest actresses . her ability to tap into the psyche and personality of the characters she portrays is very uncanny and i hope to see her win an oscar ( hell , i think she might pull off getting a best supporting actress nod for this one if the script were a little better ) . the film starts off as a psychological thriller , but a predictable one at that . if your curious to know the film's ending and twists , then see the film otherwise i would rent another predictable thriller called \" hide and seek \" .Â»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dbow,d100,hs,mc2,s0.001,t8):\n",
      "\n",
      "MOST ('train_pos_10007', 0.5079358816146851): Â«the night listener ( 2006 ) **1/2 robin williams , toni collette , bobby cannavale , rory culkin , joe morton , sandra oh , john cullum , lisa emery , becky ann baker . ( dir : patrick stettner ) hitchcockian suspenser gives williams a stand-out low-key performance . what is it about celebrities and fans ? what is the near paranoia one associates with the other and why is it almost the norm ? in the latest derange fan scenario , based on true events no less , williams stars as a talk-radio personality named gabriel no one , who reads stories he's penned over the airwaves and has accumulated an interesting fan in the form of a young boy named pete logand ( culkin ) who has submitted a manuscript about the travails of his troubled youth to no one's editor ashe ( morton ) who gives it to no one to read for himself . no one is naturally disturbed but ultimately intrigued about the nightmarish existence of pete being abducted and sexually abused for years until he was finally rescued by a nurse named donna ( collette giving an excellent performance ) who has adopted the boy but her correspondence with no one reveals that pete is dying from aids . naturally no one wants to meet the fans but is suddenly in doubt to their possibly devious ulterior motives when the seed is planted by his estranged lover jess ( cannavale ) whose sudden departure from their new york city apartment has no one in an emotional tailspin that has only now grown into a tempest in a teacup when he decides to do some investigating into donna and pete's backgrounds discovering some truths that he didn't anticipate . written by armistead maupin ( who co-wrote the screenplay with his former lover terry anderson and the film's novice director stettner ) and based on a true story about a fan's hoax found out has some hitchcockian moments that run on full tilt like any good old fashioned pot-boiler does . it helps that williams gives a stand-out , low-key performance as the conflicted good-hearted personality who genuinely wants to believe that his number one fan is in fact real and does love him ( the one thing that has escaped his own reality ) and has some unsettling dreadful moments with the creepy collette whose one physical trait i will leave unmentioned but underlines the desperation of her character that can rattle you to the core . however the film runs out of gas and eventually becomes a bit repetitive and predictable despite a finely directed piece of hoodwink and mystery by stettner , it pays to listen to your own inner voice : be careful of what you hope for .Â»\n",
      "\n",
      "MEDIAN ('train_neg_934', 0.04511338472366333): Â«macy , ullman and sutherland were as great as usual . ritter wasn't bad either . what's her name was as pretty as usual . it could have been a good movie . to bad the plot was atrocious . it was completely predictable , trite and boring . from the first 15 minutes , the rest of the movie was laid out like a child's paint by numbers routine . the characters were stock pieces of cut out cardboard . there was nothing new or interesting to say and that completely outweighed the acting , which was a pity . finally , too bad the script writer wasn't the victim . especially with the \" precocious \" lines from the child , which were completely unbelievable . again , it's only the acting that prevented a much lower score .Â»\n",
      "\n",
      "LEAST ('train_unsup_28205', -0.37922921776771545): Â«i saw it in 1969 and will never forget it . the cast was a fine cross section of the best pommie comedy actors of the period . the sight of marty feldman in a nurses uniform with crossed bandoliers of syringes was surpassed only by harry secombes ode to the pin up . would love to get it on video - does anyone know how we can get it onto cd , video , whatever .Â»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "doc_id = np.random.randint(simple_models[0].docvecs.count)  # pick random doc, re-run cell for more examples\n",
    "model = random.choice(simple_models)  # and a random model\n",
    "sims = model.docvecs.most_similar(doc_id, topn=model.docvecs.count)  # get *all* similar documents\n",
    "print(f'TARGET ({doc_id}): Â«{\" \".join(train_docs[doc_id].words)}Â»\\n')\n",
    "print(f'SIMILAR/DISSIMILAR DOCS PER MODEL {model}:\\n')\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    doc = next(doc for doc in train_docs if doc.tags[0] == sims[index][0])\n",
    "    print(f'{label} {sims[index]}: Â«{\" \".join(doc.words)}Â»\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "(Somewhat, in terms of reviewer tone, movie genre, etc... the MOST cosine-similar docs usually seem more like the TARGET than the MEDIAN or LEAST.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Do the word vectors show useful similarities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_models = simple_models[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most similar words for 'comedy/drama' (34 occurences)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8)</th><th>Doc2Vec(dbow,d100,hs,mc2,s0.001,t8)</th><th>Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8)</th></tr><tr><td>[('comedy', 0.6131592988967896),<br>\n",
       "('thriller', 0.5856417417526245),<br>\n",
       "('chiller', 0.5728679299354553),<br>\n",
       "('farce', 0.5711425542831421),<br>\n",
       "('melodrama', 0.5277600288391113),<br>\n",
       "('mystery/comedy', 0.520356297492981),<br>\n",
       "('drama', 0.5065547823905945),<br>\n",
       "('documentary', 0.4931820333003998),<br>\n",
       "('caper', 0.49190014600753784),<br>\n",
       "('whodunit', 0.477710485458374),<br>\n",
       "('drama/comedy', 0.47550463676452637),<br>\n",
       "('romp', 0.4654447138309479),<br>\n",
       "('k-drama', 0.463994562625885),<br>\n",
       "('swashbuckler', 0.46004506945610046),<br>\n",
       "('subplot', 0.4448304772377014),<br>\n",
       "('cartoon', 0.43144601583480835),<br>\n",
       "('tale', 0.4301145076751709),<br>\n",
       "('b-movie', 0.4290980100631714),<br>\n",
       "('ditty', 0.42506682872772217),<br>\n",
       "('biopic', 0.42409414052963257)]</td><td>[(\"kwouk's\", 0.3979438543319702),<br>\n",
       "('cardiovascular', 0.382252961397171),<br>\n",
       "(\"singleton's\", 0.380048006772995),<br>\n",
       "('irrigation', 0.3782654404640198),<br>\n",
       "('argento', 0.3746173679828644),<br>\n",
       "('dementedly', 0.3717617690563202),<br>\n",
       "('seema', 0.37132272124290466),<br>\n",
       "('screwfly', 0.37029075622558594),<br>\n",
       "('rhetoric', 0.36829674243927),<br>\n",
       "(\"'accident\", 0.3648317754268646),<br>\n",
       "('non-ending', 0.357203871011734),<br>\n",
       "('persisting', 0.3563710153102875),<br>\n",
       "('amore', 0.356012761592865),<br>\n",
       "(\"jet's\", 0.3543563485145569),<br>\n",
       "('achmed', 0.349098265171051),<br>\n",
       "('ossie', 0.34883469343185425),<br>\n",
       "('mish-mash', 0.3478613495826721),<br>\n",
       "('2-3', 0.34463465213775635),<br>\n",
       "('irving', 0.3430761396884918),<br>\n",
       "('marge', 0.34153223037719727)]</td><td>[('comedy', 0.47908416390419006),<br>\n",
       "('comedy-drama', 0.46982747316360474),<br>\n",
       "('drama', 0.42911234498023987),<br>\n",
       "('regress', 0.42287036776542664),<br>\n",
       "('diversion', 0.4086240530014038),<br>\n",
       "('undiscriminating', 0.4037162661552429),<br>\n",
       "(\"'slice-of-life'\", 0.4026561379432678),<br>\n",
       "('thriller', 0.39599186182022095),<br>\n",
       "('farce', 0.38499534130096436),<br>\n",
       "('potboiler', 0.38408589363098145),<br>\n",
       "('kar-kui', 0.381378710269928),<br>\n",
       "('pangborn', 0.37826529145240784),<br>\n",
       "('caper', 0.37742534279823303),<br>\n",
       "('horror/thriller', 0.37568849325180054),<br>\n",
       "('entertainer', 0.3728658854961395),<br>\n",
       "('aerospace', 0.3692465126514435),<br>\n",
       "('lv2', 0.36792242527008057),<br>\n",
       "('promos', 0.3663831949234009),<br>\n",
       "('sissle', 0.3624805510044098),<br>\n",
       "('denominator', 0.3614151179790497)]</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from IPython.display import HTML\n",
    "# pick a random word with a suitable number of occurences\n",
    "while True:\n",
    "    word = random.choice(word_models[0].wv.index2word)\n",
    "    if word_models[0].wv.vocab[word].count > 10:\n",
    "        break\n",
    "# or uncomment below line, to just pick a word from the relevant domain:\n",
    "word = 'comedy/drama'\n",
    "similars_per_model = [str(model.most_similar(word, topn=20)).replace('), ','),<br>\\n') for model in word_models]\n",
    "similar_table = (\"<table><tr><th>\" +\n",
    "    \"</th><th>\".join([str(model) for model in word_models]) + \n",
    "    \"</th></tr><tr><td>\" +\n",
    "    \"</td><td>\".join(similars_per_model) +\n",
    "    \"</td></tr></table>\")\n",
    "print(\"most similar words for '%s' (%d occurences)\" % (word, simple_models[0].wv.vocab[word].count))\n",
    "HTML(similar_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Do the DBOW words look meaningless? That's because the gensim DBOW model doesn't train word vectors â€“ they remain at their random initialized values â€“ unless you ask with the `dbow_words=1` initialization parameter. Concurrent word-training slows DBOW mode significantly, and offers little improvement (and sometimes a little worsening) of the error rate on this IMDB sentiment-prediction task. \n",
    "\n",
    "Words from DM models tend to show meaningfully similar words when there are many examples in the training data (as with 'plot' or 'actor'). (All DM modes inherently involve word vector training concurrent with doc vector training.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Are the word vectors from this dataset any good at analogies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# assuming something like\n",
    "# https://word2vec.googlecode.com/svn/trunk/questions-words.txt \n",
    "# is in local directory\n",
    "# note: this takes many minutes\n",
    "for model in word_models:\n",
    "    sections = model.accuracy('questions-words.txt')\n",
    "    correct, incorrect = len(sections[-1]['correct']), len(sections[-1]['incorrect'])\n",
    "    print('%s: %0.2f%% correct (%d of %d)' % (model, float(correct*100)/(correct+incorrect), correct, correct+incorrect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Even though this is a tiny, domain-specific dataset, it shows some meager capability on the general word analogies â€“ at least for the DM/concat and DM/mean models which actually train word vectors. (The untrained random-initialized words of the DBOW model of course fail miserably.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Slop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "This cell left intentionally erroneous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To mix the Google dataset (if locally available) into the word tests..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "w2v_g100b = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "w2v_g100b.compact_name = 'w2v_g100b'\n",
    "word_models.append(w2v_g100b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To get copious logging output from above steps..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "rootLogger = logging.getLogger()\n",
    "rootLogger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To auto-reload python code while developing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
