{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# gensim doc2vec & IMDB sentiment dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "TODO: section on introduction & motivation\n",
    "\n",
    "TODO: prerequisites + dependencies (statsmodels, patsy, ?)\n",
    "\n",
    "### Requirements\n",
    "Following are the dependencies for this tutorial:\n",
    "    - testfixtures\n",
    "    - statsmodels\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Fetch and prep exactly as in Mikolov's go.sh shell script. (Note this cell tests for existence of required files, so steps won't repeat once the final summary file (`aclImdb/alldata-id.txt`) is available alongside this notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "files = ['train_pos', 'train_neg', 'test_pos', 'test_neg', 'train_unsup']\n",
    "\n",
    "with open('../data/imdb_sentiment/alldata-id.txt', 'w', encoding='utf-8') as out_sr:\n",
    "    i = 0\n",
    "    for file in files:\n",
    "        with open(os.path.join('../data/imdb_sentiment', f'{file}.txt'), 'r', encoding='utf-8') as in_sr:\n",
    "            for line in in_sr:\n",
    "                out_sr.write(f'_*{i} {line}')\n",
    "                i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The data is small enough to be read into memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 docs: 25000 train-sentiment, 25000 test-sentiment\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from collections import namedtuple\n",
    "\n",
    "SentimentDocument = namedtuple('SentimentDocument', 'words tags split sentiment')\n",
    "\n",
    "alldocs = []  # will hold all docs in original order\n",
    "with open('../data/imdb_sentiment/alldata-id.txt', encoding='utf-8') as alldata:\n",
    "    for line_no, line in enumerate(alldata):\n",
    "        tokens = line.split()\n",
    "        words = tokens[1:]\n",
    "        tags = [line_no] # `tags = [tokens[0]]` would also work at extra memory cost\n",
    "        split = ['train','test','extra','extra'][line_no//25000]  # 25k train, 25k test, 25k extra\n",
    "        sentiment = [1.0, 0.0, 1.0, 0.0, None, None, None, None][line_no//12500] # [12.5K pos, 12.5K neg]*2 then unknown\n",
    "        alldocs.append(SentimentDocument(words, tags, split, sentiment))\n",
    "\n",
    "train_docs = [doc for doc in alldocs if doc.split == 'train']\n",
    "test_docs = [doc for doc in alldocs if doc.split == 'test']\n",
    "doc_list = alldocs[:]  # for reshuffling per pass\n",
    "\n",
    "print('%d docs: %d train-sentiment, %d test-sentiment' % (len(doc_list), len(train_docs), len(test_docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Set-up Doc2Vec Training & Evaluation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Approximating experiment of Le & Mikolov [\"Distributed Representations of Sentences and Documents\"](http://cs.stanford.edu/~quocle/paragraph_vector.pdf), also with guidance from Mikolov's [example go.sh](https://groups.google.com/d/msg/word2vec-toolkit/Q49FIrNOQRo/J6KG8mUj45sJ):\n",
    "\n",
    "`./word2vec -train ../alldata-id.txt -output vectors.txt -cbow 0 -size 100 -window 10 -negative 5 -hs 0 -sample 1e-4 -threads 40 -binary 0 -iter 20 -min-count 1 -sentence-vectors 1`\n",
    "\n",
    "Parameter choices below vary:\n",
    "\n",
    "* 100-dimensional vectors, as the 400d vectors of the paper don't seem to offer much benefit on this task\n",
    "* similarly, frequent word subsampling seems to decrease sentiment-prediction accuracy, so it's left out\n",
    "* `cbow=0` means skip-gram which is equivalent to the paper's 'PV-DBOW' mode, matched in gensim with `dm=0`\n",
    "* added to that DBOW model are two DM models, one which averages context vectors (`dm_mean`) and one which concatenates them (`dm_concat`, resulting in a much larger, slower, more data-hungry model)\n",
    "* a `min_count=2` saves quite a bit of model memory, discarding only words that appear in a single doc (and are thus no more expressive than the unique-to-each doc vectors themselves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8)\n",
      "Doc2Vec(dbow,d100,hs,mc2,s0.001,t8)\n",
      "Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "from collections import OrderedDict\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"this will be painfully slow otherwise\"\n",
    "\n",
    "simple_models = [\n",
    "    # PV-DM w/concatenation - window=5 (both sides) approximates paper's 10-word total window size\n",
    "    Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=0, hs=1, min_count=2, workers=cores, iter=1),\n",
    "    # PV-DBOW \n",
    "    Doc2Vec(dm=0, size=100, negative=0, hs=1, min_count=2, workers=cores, iter=1),\n",
    "    # PV-DM w/average\n",
    "    Doc2Vec(dm=1, dm_mean=1, size=100, window=10, negative=0, hs=1, min_count=2, workers=cores, iter=1),\n",
    "]\n",
    "\n",
    "for model in simple_models:\n",
    "    model.build_vocab(alldocs)\n",
    "    print(model)\n",
    "\n",
    "models_by_name = OrderedDict((str(model), model) for model in simple_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Following the paper, we also evaluate models in pairs. These wrappers return the concatenation of the vectors from each model. (Only the singular models are trained.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "models_by_name['dbow+dmm'] = ConcatenatedDoc2Vec([simple_models[1], simple_models[2]])\n",
    "models_by_name['dbow+dmc'] = ConcatenatedDoc2Vec([simple_models[1], simple_models[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Predictive Evaluation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Helper methods for evaluating error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from random import sample\n",
    "\n",
    "# for timing\n",
    "from contextlib import contextmanager\n",
    "from timeit import default_timer\n",
    "import time \n",
    "\n",
    "@contextmanager\n",
    "def elapsed_timer():\n",
    "    start = default_timer()\n",
    "    elapser = lambda: default_timer() - start\n",
    "    yield lambda: elapser()\n",
    "    end = default_timer()\n",
    "    elapser = lambda: end-start\n",
    "    \n",
    "def logistic_predictor_from_data(train_targets, train_regressors):\n",
    "    logit = sm.Logit(train_targets, train_regressors)\n",
    "    predictor = logit.fit(disp=0)\n",
    "    #print(predictor.summary())\n",
    "    return predictor\n",
    "\n",
    "def error_rate_for_model(test_model, train_set, test_set, infer=False, infer_steps=3, infer_alpha=0.1, infer_subsample=0.1):\n",
    "    \"\"\"Report error rate on test_doc sentiments, using supplied model and train_docs\"\"\"\n",
    "\n",
    "    train_targets, train_regressors = zip(*[(doc.sentiment, test_model.docvecs[doc.tags[0]]) for doc in train_set])\n",
    "    train_regressors = sm.add_constant(train_regressors)\n",
    "    predictor = logistic_predictor_from_data(train_targets, train_regressors)\n",
    "\n",
    "    test_data = test_set\n",
    "    if infer:\n",
    "        if infer_subsample < 1.0:\n",
    "            test_data = sample(test_data, int(infer_subsample * len(test_data)))\n",
    "        test_regressors = [test_model.infer_vector(doc.words, steps=infer_steps, alpha=infer_alpha) for doc in test_data]\n",
    "    else:\n",
    "        test_regressors = [test_model.docvecs[doc.tags[0]] for doc in test_docs]\n",
    "    test_regressors = sm.add_constant(test_regressors)\n",
    "    \n",
    "    # predict & evaluate\n",
    "    test_predictions = predictor.predict(test_regressors)\n",
    "    corrects = sum(np.rint(test_predictions) == [doc.sentiment for doc in test_data])\n",
    "    errors = len(test_predictions) - corrects\n",
    "    error_rate = float(errors) / len(test_predictions)\n",
    "    return (error_rate, errors, len(test_predictions), predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Bulk Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Using explicit multiple-pass, alpha-reduction approach as sketched in [gensim doc2vec blog post](http://radimrehurek.com/2014/12/doc2vec-tutorial/) – with added shuffling of corpus on each pass.\n",
    "\n",
    "Note that vector training is occurring on *all* documents of the dataset, which includes all TRAIN/TEST/DEV docs.\n",
    "\n",
    "Evaluation of each model's sentiment-predictive power is repeated after each pass, as an error rate (lower is better), to see the rates-of-relative-improvement. The base numbers reuse the TRAIN and TEST vectors stored in the models for the logistic regression, while the _inferred_ results use newly-inferred TEST vectors. \n",
    "\n",
    "(On a 4-core 2.6Ghz Intel Core i7, these 20 passes training and evaluating 3 main models takes about an hour.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "best_error = defaultdict(lambda :1.0)  # to selectively-print only best errors achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START 2017-04-01 18:54:30.865517\n",
      "*0.300800 : 1 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8) 43.8s 0.5s\n",
      "*0.259600 : 1 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8)_inferred 43.8s 7.8s\n",
      "*0.192800 : 1 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8) 14.3s 0.9s\n",
      "*0.169600 : 1 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8)_inferred 14.3s 2.3s\n",
      "*0.248000 : 1 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8) 21.0s 0.5s\n",
      "*0.230800 : 1 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8)_inferred 21.0s 3.4s\n",
      "*0.185480 : 1 passes : dbow+dmm 0.0s 1.4s\n",
      "*0.170000 : 1 passes : dbow+dmm_inferred 0.0s 6.3s\n",
      "*0.193280 : 1 passes : dbow+dmc 0.0s 1.5s\n",
      "*0.190800 : 1 passes : dbow+dmc_inferred 0.0s 11.0s\n",
      "completed pass 1 at alpha 0.025000\n",
      "*0.241320 : 2 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8) 43.7s 0.5s\n",
      "*0.141160 : 2 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8) 13.6s 0.5s\n",
      "*0.208080 : 2 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8) 19.9s 0.9s\n",
      "*0.136000 : 2 passes : dbow+dmm 0.0s 1.5s\n",
      "*0.140080 : 2 passes : dbow+dmc 0.0s 1.5s\n",
      "completed pass 2 at alpha 0.023800\n",
      "*0.219680 : 3 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8) 43.4s 0.5s\n",
      "*0.128080 : 3 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8) 14.4s 0.6s\n",
      "*0.196320 : 3 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8) 20.5s 0.6s\n",
      "*0.124360 : 3 passes : dbow+dmm 0.0s 1.5s\n",
      "*0.126600 : 3 passes : dbow+dmc 0.0s 1.6s\n",
      "completed pass 3 at alpha 0.022600\n",
      "*0.206680 : 4 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8) 43.1s 0.5s\n",
      "*0.120320 : 4 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8) 14.6s 0.6s\n",
      "*0.189320 : 4 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8) 21.0s 1.0s\n",
      "*0.117080 : 4 passes : dbow+dmm 0.0s 1.6s\n",
      "*0.120240 : 4 passes : dbow+dmc 0.0s 1.6s\n",
      "completed pass 4 at alpha 0.021400\n",
      "*0.200920 : 5 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8) 43.8s 0.5s\n",
      "*0.210000 : 5 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8)_inferred 43.8s 7.6s\n",
      "*0.118120 : 5 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8) 13.7s 0.6s\n",
      "*0.114400 : 5 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8)_inferred 13.7s 2.5s\n",
      "*0.184000 : 5 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8) 20.4s 0.6s\n",
      "*0.182400 : 5 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8)_inferred 20.4s 3.3s\n",
      "*0.115320 : 5 passes : dbow+dmm 0.0s 2.1s\n",
      "*0.130800 : 5 passes : dbow+dmm_inferred 0.0s 6.7s\n",
      "*0.117200 : 5 passes : dbow+dmc 0.0s 1.5s\n",
      "*0.120800 : 5 passes : dbow+dmc_inferred 0.0s 11.4s\n",
      "completed pass 5 at alpha 0.020200\n",
      "*0.196920 : 6 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8) 44.7s 0.5s\n",
      "*0.116600 : 6 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8) 13.8s 0.5s\n",
      "*0.180840 : 6 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8) 20.5s 0.5s\n",
      "*0.113960 : 6 passes : dbow+dmm 0.0s 1.6s\n",
      "*0.115520 : 6 passes : dbow+dmc 0.0s 1.6s\n",
      "completed pass 6 at alpha 0.019000\n",
      "*0.193320 : 7 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8) 43.4s 0.5s\n",
      "*0.115960 : 7 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8) 13.4s 1.0s\n",
      "*0.178320 : 7 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8) 21.3s 0.5s\n",
      "*0.113880 : 7 passes : dbow+dmm 0.0s 1.5s\n",
      "*0.114040 : 7 passes : dbow+dmc 0.0s 1.5s\n",
      "completed pass 7 at alpha 0.017800\n",
      "*0.190440 : 8 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8) 42.4s 0.5s\n",
      "*0.115160 : 8 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8) 14.9s 0.5s\n",
      "*0.176800 : 8 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8) 20.6s 0.5s\n",
      "*0.113160 : 8 passes : dbow+dmm 0.0s 1.5s\n",
      " 0.115040 : 8 passes : dbow+dmc 0.0s 1.6s\n",
      "completed pass 8 at alpha 0.016600\n",
      "*0.189400 : 9 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8) 43.9s 0.5s\n",
      " 0.115520 : 9 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8) 13.6s 0.5s\n",
      "*0.176320 : 9 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8) 21.4s 0.9s\n",
      " 0.114600 : 9 passes : dbow+dmm 0.0s 1.7s\n",
      " 0.115800 : 9 passes : dbow+dmc 0.0s 1.6s\n",
      "completed pass 9 at alpha 0.015400\n",
      "*0.188600 : 10 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8) 45.9s 0.5s\n",
      "*0.186000 : 10 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8)_inferred 45.9s 7.4s\n",
      " 0.115400 : 10 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8) 13.4s 0.6s\n",
      " 0.121600 : 10 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8)_inferred 13.4s 2.4s\n",
      "*0.174560 : 10 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8) 21.0s 0.5s\n",
      "*0.174400 : 10 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8)_inferred 21.0s 3.4s\n",
      " 0.114840 : 10 passes : dbow+dmm 0.0s 1.6s\n",
      "*0.114800 : 10 passes : dbow+dmm_inferred 0.0s 6.7s\n",
      " 0.115440 : 10 passes : dbow+dmc 0.0s 1.5s\n",
      "*0.112800 : 10 passes : dbow+dmc_inferred 0.0s 10.9s\n",
      "completed pass 10 at alpha 0.014200\n",
      "*0.187640 : 11 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8) 42.1s 0.5s\n",
      " 0.116680 : 11 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8) 13.9s 0.5s\n",
      "*0.173200 : 11 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8) 20.3s 0.6s\n",
      " 0.115440 : 11 passes : dbow+dmm 0.0s 1.5s\n",
      " 0.115960 : 11 passes : dbow+dmc 0.0s 1.7s\n",
      "completed pass 11 at alpha 0.013000\n",
      "*0.185920 : 12 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8) 41.6s 0.5s\n",
      " 0.116200 : 12 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8) 13.5s 0.9s\n",
      "*0.172000 : 12 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8) 19.7s 0.5s\n",
      " 0.115280 : 12 passes : dbow+dmm 0.0s 1.5s\n",
      " 0.115960 : 12 passes : dbow+dmc 0.0s 1.5s\n",
      "completed pass 12 at alpha 0.011800\n",
      " 0.187120 : 13 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8) 43.2s 0.5s\n",
      " 0.116800 : 13 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8) 13.7s 0.6s\n",
      "*0.171080 : 13 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8) 19.4s 0.5s\n",
      " 0.115400 : 13 passes : dbow+dmm 0.0s 1.5s\n",
      " 0.117040 : 13 passes : dbow+dmc 0.0s 1.4s\n",
      "completed pass 13 at alpha 0.010600\n",
      " 0.188280 : 14 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8) 40.2s 0.5s\n",
      " 0.116560 : 14 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8) 13.0s 1.0s\n",
      "*0.170920 : 14 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8) 19.3s 0.5s\n",
      " 0.114960 : 14 passes : dbow+dmm 0.0s 1.5s\n",
      " 0.116720 : 14 passes : dbow+dmc 0.0s 1.5s\n",
      "completed pass 14 at alpha 0.009400\n",
      " 0.188320 : 15 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8) 41.6s 0.5s\n",
      " 0.209200 : 15 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8)_inferred 41.6s 8.1s\n",
      " 0.116400 : 15 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8) 13.4s 0.5s\n",
      " 0.129600 : 15 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8)_inferred 13.4s 2.4s\n",
      "*0.170640 : 15 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8) 23.9s 0.6s\n",
      " 0.182400 : 15 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8)_inferred 23.9s 3.9s\n",
      " 0.115120 : 15 passes : dbow+dmm 0.0s 1.8s\n",
      "*0.108800 : 15 passes : dbow+dmm_inferred 0.0s 6.4s\n",
      " 0.116400 : 15 passes : dbow+dmc 0.0s 1.6s\n",
      " 0.126800 : 15 passes : dbow+dmc_inferred 0.0s 11.4s\n",
      "completed pass 15 at alpha 0.008200\n",
      " 0.188840 : 16 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8) 43.8s 0.5s\n",
      " 0.116320 : 16 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8) 13.8s 0.6s\n",
      "*0.169720 : 16 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8) 20.1s 0.5s\n",
      " 0.115800 : 16 passes : dbow+dmm 0.0s 1.6s\n",
      " 0.117240 : 16 passes : dbow+dmc 0.0s 1.6s\n",
      "completed pass 16 at alpha 0.007000\n",
      " 0.188920 : 17 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8) 41.7s 0.9s\n",
      " 0.116360 : 17 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8) 13.6s 0.5s\n",
      "*0.169200 : 17 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8) 20.1s 0.5s\n",
      " 0.115560 : 17 passes : dbow+dmm 0.0s 1.5s\n",
      " 0.117600 : 17 passes : dbow+dmc 0.0s 1.6s\n",
      "completed pass 17 at alpha 0.005800\n",
      " 0.189440 : 18 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8) 42.7s 0.6s\n",
      " 0.116520 : 18 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8) 14.6s 0.5s\n",
      "*0.168520 : 18 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8) 19.4s 0.5s\n",
      " 0.116240 : 18 passes : dbow+dmm 0.0s 1.5s\n",
      " 0.117800 : 18 passes : dbow+dmc 0.0s 1.5s\n",
      "completed pass 18 at alpha 0.004600\n",
      " 0.190040 : 19 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8) 40.4s 0.9s\n",
      " 0.116840 : 19 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8) 13.6s 0.5s\n",
      " 0.168760 : 19 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8) 20.6s 0.6s\n",
      " 0.116240 : 19 passes : dbow+dmm 0.0s 1.7s\n",
      " 0.117960 : 19 passes : dbow+dmc 0.0s 1.8s\n",
      "completed pass 19 at alpha 0.003400\n",
      " 0.190200 : 20 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8) 42.4s 0.5s\n",
      " 0.194400 : 20 passes : Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8)_inferred 42.4s 7.8s\n",
      " 0.117080 : 20 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8) 13.7s 0.5s\n",
      " 0.114800 : 20 passes : Doc2Vec(dbow,d100,hs,mc2,s0.001,t8)_inferred 13.7s 2.4s\n",
      "*0.168240 : 20 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8) 20.5s 0.5s\n",
      "*0.163600 : 20 passes : Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8)_inferred 20.5s 4.1s\n",
      " 0.116800 : 20 passes : dbow+dmm 0.0s 1.5s\n",
      " 0.118400 : 20 passes : dbow+dmm_inferred 0.0s 6.1s\n",
      " 0.118120 : 20 passes : dbow+dmc 0.0s 1.6s\n",
      " 0.117600 : 20 passes : dbow+dmc_inferred 0.0s 10.7s\n",
      "completed pass 20 at alpha 0.002200\n",
      "END 2017-04-01 19:24:32.339154\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "import datetime\n",
    "\n",
    "alpha, min_alpha, passes = (0.025, 0.001, 20)\n",
    "alpha_delta = (alpha - min_alpha) / passes\n",
    "\n",
    "print(\"START %s\" % datetime.datetime.now())\n",
    "\n",
    "for epoch in range(passes):\n",
    "    shuffle(doc_list)  # shuffling gets best results\n",
    "    \n",
    "    for name, train_model in models_by_name.items():\n",
    "        # train\n",
    "        duration = 'na'\n",
    "        train_model.alpha, train_model.min_alpha = alpha, alpha\n",
    "        with elapsed_timer() as elapsed:\n",
    "            train_model.train(doc_list)\n",
    "            duration = '%.1f' % elapsed()\n",
    "            \n",
    "        # evaluate\n",
    "        eval_duration = ''\n",
    "        with elapsed_timer() as eval_elapsed:\n",
    "            err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs, test_docs)\n",
    "        eval_duration = '%.1f' % eval_elapsed()\n",
    "        best_indicator = ' '\n",
    "        if err <= best_error[name]:\n",
    "            best_error[name] = err\n",
    "            best_indicator = '*' \n",
    "        print(\"%s%f : %i passes : %s %ss %ss\" % (best_indicator, err, epoch + 1, name, duration, eval_duration))\n",
    "\n",
    "        if ((epoch + 1) % 5) == 0 or epoch == 0:\n",
    "            eval_duration = ''\n",
    "            with elapsed_timer() as eval_elapsed:\n",
    "                infer_err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs, test_docs, infer=True)\n",
    "            eval_duration = '%.1f' % eval_elapsed()\n",
    "            best_indicator = ' '\n",
    "            if infer_err < best_error[name + '_inferred']:\n",
    "                best_error[name + '_inferred'] = infer_err\n",
    "                best_indicator = '*'\n",
    "            print(\"%s%f : %i passes : %s %ss %ss\" % (best_indicator, infer_err, epoch + 1, name + '_inferred', duration, eval_duration))\n",
    "\n",
    "    print('completed pass %i at alpha %f' % (epoch + 1, alpha))\n",
    "    alpha -= alpha_delta\n",
    "    \n",
    "print(\"END %s\" % str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Achieved Sentiment-Prediction Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.108800 dbow+dmm_inferred\n",
      "0.112800 dbow+dmc_inferred\n",
      "0.113160 dbow+dmm\n",
      "0.114040 dbow+dmc\n",
      "0.114400 Doc2Vec(dbow,d100,hs,mc2,s0.001,t8)_inferred\n",
      "0.115160 Doc2Vec(dbow,d100,hs,mc2,s0.001,t8)\n",
      "0.163600 Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8)_inferred\n",
      "0.168240 Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8)\n",
      "0.185920 Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8)\n",
      "0.186000 Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8)_inferred\n"
     ]
    }
   ],
   "source": [
    "# print best error rates achieved\n",
    "for rate, name in sorted((rate, name) for name, rate in best_error.items()):\n",
    "    print(\"%f %s\" % (rate, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In my testing, unlike the paper's report, DBOW performs best. Concatenating vectors from different models only offers a small predictive improvement. The best results I've seen are still just under 10% error rate, still a ways from the paper's 7.42%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Examining Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Are inferred vectors close to the precalculated ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for doc 2266...\n",
      "Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8):\n",
      " [(2266, 0.98240065574646), (72962, 0.4310319423675537), (54219, 0.4261874556541443)]\n",
      "Doc2Vec(dbow,d100,hs,mc2,s0.001,t8):\n",
      " [(2266, 0.9899259209632874), (79639, 0.4902854263782501), (96035, 0.4768390357494354)]\n",
      "Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8):\n",
      " [(2266, 0.9670548439025879), (27671, 0.43493229150772095), (67923, 0.4335106909275055)]\n"
     ]
    }
   ],
   "source": [
    "doc_id = np.random.randint(simple_models[0].docvecs.count)  # pick random doc; re-run cell for more examples\n",
    "print('for doc %d...' % doc_id)\n",
    "for model in simple_models:\n",
    "    inferred_docvec = model.infer_vector(alldocs[doc_id].words)\n",
    "    print('%s:\\n %s' % (model, model.docvecs.most_similar([inferred_docvec], topn=3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "(Yes, here the stored vector from 20 epochs of training is usually one of the closest to a freshly-inferred vector for the same words. Note the defaults for inference are very abbreviated – just 3 steps starting at a high alpha – and likely need tuning for other applications.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Do close documents seem more related than distant ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET (12505): «i basically skimmed through the movie but just enough to catch watch the plot was about . to tell you the truth it was kind of boring to me and at some spots it didn't make sense . the only reason i watched this movie in the first place was to see chace crawford ! ! ! he is so hot , but in this movie his hair was kind of weird . but still hot . however , despite how hot chace is , it really did not make up for the film . i guess the plot isn't that bad but what really threw me over was the fact that they cuss in like every sentence . is it that hard to express your anger without saying the f word every time ? the cussing was annoying and the whole flashy , camera shaking thing gave me a headache . all in all , although the plot was ok , i found the film to be a bore and over dramatic . that's why i only cut to scenes with chace in it . lol anyways , not worth renting unless your a die-hard fan of a specific cast member like i was . oh yeah the cast was hot . the girls were hot ! ! ! but chace is the best ! !»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8):\n",
      "\n",
      "MOST (407, 0.42833858728408813): «o my gosh . . . just give me a minute to breath . this movie takes you on an awesome ride and doesn't let you go until the very last blow in your face ending . this is the the movie for fans of stormriders and such . legend of zu was beautifully created , although i didn't like a few things , they used alot of stand ins . i wanted to see the real person fight but , oh well . . . a few small let downs , and i didn't really like it a lot until i watched it again , when you understand it more it totally kicks a** ! i encourage anyone who ever wanted to see a true asian movie , to see this movie . i give this movie one of my highest ratings . go and see it when it comes out in america ! ! !»\n",
      "\n",
      "MEDIAN (21453, -0.0020360760390758514): «i saw this movie literally directly after finishing the book , and maybe that was a neutral idea or a very stupid one . i think it was the latter . first of all , it was inaccurate in many small , yet important details . one of the first things i noticed was , during winston's day to day life in his work , his conversations , eating in the cafeteria , etc . he feels free to look unhappy and make suggestive glances at people without immense fear . one of the most important parts of the book , was that even in small activities it was virtually impossible to safely show even a hint of his true emotions on his face at any moment . this is also shown in the scenes on the streets of the proletarions . in the book winston knew that this was a huge risk to wander around there and was skeptical and frightened at every trip . while in the movie , he does it so often and without fear , that you lose the important feeling of heavy surveillance and risk right off the bat . other minor inaccuracies included winston hiding his diary in the wall , yes a very small change , but it begs the question , what's the point ? there was also the most annoying thing a director can do with a book , and that is morphing characters . the large inaccuracies were far more disturbing , however . first of all , one of the important pieces of the book is that big brother is a government based on an intelligent , yet crude philosophy . in the movie , they skip that and go straight to making you think that the government is run by hitler with technology . which is true , in a sense , when directed with its facism , but if that's all you get out of big brother , you really missed the point of the book . the terrifying thing about big brother is that , in a way , it has some points behind its philosophy . when o'brien is picking at winstons mind in the ministry of love , he is listening to everything winston says against big brother . the fact that he listens , and advances forward in his philosophy , is in effect what is most creepy and intriguing . in the end , ( careful spoiler ahead ) when winston says he loves big brother , the terrifying thing is that you are not sure whether it was souly the beating and torture that caused this , or the actual power behind the philosophy . i am in no way saying that the big brother's philosophy has points that appeal to me , but its intelligence and depth is what makes this book incredibly disturbing . also , how could anyone feel any connection between julia and winston in the film ? it was awful , no connection whatsoever . and where was o'brien before he gave winston his address ? one of the things that carried the book was winstons thoughts about o'brien before he made contact with him . in the movie , they just jump the gun . but that about sums up why this movie was a terrible adaption : because its impossible not to jump the gun and morph characters in less than two hours . how could anyone think this movie was watchable if it was under two hours ? at the very least , the movie demands 3 hours to be able to capture some of the important moods and connections . anything less is just pointless . if you loved the book , and i mean truly adored it , you will not approve of this movie , and chances are , you already knew you wouldn't . because the book is unfilmable , and this movie just proves how impossible it is cram something decent into a small reel of film . two stars out of ten»\n",
      "\n",
      "LEAST (8631, -0.4556315243244171): «\" like the first touch of pleasure and guilt , like a spontaneous youthful flirt of fascination and fear , like a climax of contrary emotions \" said one of the movie buffs after viewing love at the top , the misinterpreted title version of stylish director michel deville's le mouton enrage . vincent canby in new york times , however , just after the 1974 premiere of the movie stated : \" love at the top which opened yesterday at the 68th street playhouse , is a 1973 french comedy that dimly recalls a number of nineteen-fifties english comedies about the rise and rise of cynical young men possessingand possessed byambition . \" yet , the significant difference that he mentioned was the fact that love at the top is not concerned with the english class system . . . ( january 27 , 1975 ) having left the evaluations up to single individuals , of course , the test of time has done its just job . what may be said with certainty after more than 30 years is that we can hardly find such movies like le mouton enrage where decadence appears innocent , where liaisons appear youthfully enthusiastic , where feelings occur so manipulative . for romy schneider's fans , it seems useless to point out that this film is a must see , not only because she gives a unique performance ( as she did in all of her roles ) at the heyday of her career ( 9 years before her sudden death ) but because she is particularly attractive here . it is not trio infernal where the , so to say , 'forced escape' from and the mockery of romy's sweet image haunted for years by saccharine sissi meets its most discouraging manifestation , but a film where the brilliant actress is given a fair role . she plays roberte , a woman who becomes the object of lust for the story's lead , playboy nicolas mallet ( jean louis trintignant ) . it is him who takes financial profits from lustful liaisons . this movie can boast truly memorable and unique shots of romy and she is given some of her very best scenes . romy's sex appeal is unforgettable here . another strong point of the film is its execution of the content with a development of individual perception . immoral as it may seem , the director makes a perfect use of contrast : conventions vs pleasures , innocence vs decadence , genuine lust vs instrumental affair . nicola owns most of the features that viewers may like or detest , may find attractive or disgusting ; yet , his are the features the viewers must treat seriously , more to say , they are the ones we all must accept . that is why , one is led to a peculiar , gently wild , erotically unique world of the main character . although he sleeps with lots of women , there are two women that represent a sort of contrary worlds for nicola : roberte groult ( romy schneider ) and marie-paul ( jane birkin ) . he manipulates them , makes love to them , cannot refrain from both desire for their bodies and desire for money ; yet , he perceives them differently . yet , despite all of this 'adult maturity , ' he is emotionally like a little boy who plays with a toy-car on the table - a sort of 'detailed insight into male mind . . . ' in a comedy-like way , of course . finally , there are very good performances , which makes le mouton enrage slightly underrated . not only the aforementioned romy schneider does a brilliant job supplying the viewers with an extraordinary insight into her role , but young jane birkin appears to be convincing in the role of young , inexperienced streetwalker marie paul , jean louis trintignant makes it possible to see nicola in the right way . this artistic merit lying in performances goes with terrific music by camille saint-saëns , the tune that will ring in your ears for long . therefore , apart from some flaws of the movie like dated colors , slow action ( sometimes ) , possible clichés ( noticed by some viewers ) , the merits should be found significant . le mouton enrage , in sum , is a clear manifestation of contrary manipulative tools in life . it is worth seeing as a moment in romy's career , a prelude to strong eroticism , a chain of contrary emotions , of love and hatred , appreciation and disgust compared to the first orgasm and the first angasm . . . but aren't we , humans , 'viewers , ' movie buffs built upon such contrasts ?»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "doc_id = np.random.randint(simple_models[0].docvecs.count)  # pick random doc, re-run cell for more examples\n",
    "model = random.choice(simple_models)  # and a random model\n",
    "sims = model.docvecs.most_similar(doc_id, topn=model.docvecs.count)  # get *all* similar documents\n",
    "print(u'TARGET (%d): «%s»\\n' % (doc_id, ' '.join(alldocs[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(alldocs[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "(Somewhat, in terms of reviewer tone, movie genre, etc... the MOST cosine-similar docs usually seem more like the TARGET than the MEDIAN or LEAST.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Do the word vectors show useful similarities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_models = simple_models[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most similar words for 'comedy/drama' (38 occurences)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>Doc2Vec(dm/c,d100,hs,w5,mc2,s0.001,t8)</th><th>Doc2Vec(dbow,d100,hs,mc2,s0.001,t8)</th><th>Doc2Vec(dm/m,d100,hs,w10,mc2,s0.001,t8)</th></tr><tr><td>[('comedy', 0.6122885942459106),<br>\n",
       "('farce', 0.5506311058998108),<br>\n",
       "('b-movie', 0.5445684194564819),<br>\n",
       "('thriller', 0.5365164279937744),<br>\n",
       "('curio', 0.5331360697746277),<br>\n",
       "('chiller', 0.5264720916748047),<br>\n",
       "('documentary', 0.5221627950668335),<br>\n",
       "('drama', 0.5147754549980164),<br>\n",
       "('melodrama', 0.5093796253204346),<br>\n",
       "('entertainment', 0.5034881234169006),<br>\n",
       "('epic', 0.49918028712272644),<br>\n",
       "('drama/action', 0.4817013144493103),<br>\n",
       "('adventure', 0.4715593755245209),<br>\n",
       "('yarn', 0.46752169728279114),<br>\n",
       "('giallo', 0.4619283378124237),<br>\n",
       "('diversion', 0.461751252412796),<br>\n",
       "('anthology', 0.4590337872505188),<br>\n",
       "('romp', 0.45245903730392456),<br>\n",
       "('horror/slasher', 0.45245465636253357),<br>\n",
       "('anime', 0.45210081338882446)]</td><td>[('cass', 0.41075843572616577),<br>\n",
       "('agile', 0.41023164987564087),<br>\n",
       "('flat-foot', 0.40284886956214905),<br>\n",
       "(\"murdoch's\", 0.39888352155685425),<br>\n",
       "('amputated', 0.39226916432380676),<br>\n",
       "('chronological', 0.38991647958755493),<br>\n",
       "('h2', 0.38912999629974365),<br>\n",
       "('less-than-hilarious', 0.3848978877067566),<br>\n",
       "(\"am'\", 0.38116079568862915),<br>\n",
       "('ramar', 0.3752664029598236),<br>\n",
       "('venezuelans', 0.3747956156730652),<br>\n",
       "(\"'humour'\", 0.3734762668609619),<br>\n",
       "('majorette', 0.3710702955722809),<br>\n",
       "('timbers', 0.36800745129585266),<br>\n",
       "('arnie', 0.3668188452720642),<br>\n",
       "('cleaverly', 0.3665322959423065),<br>\n",
       "('solo', 0.36620521545410156),<br>\n",
       "('blue-haired', 0.3651135265827179),<br>\n",
       "('tuxedo', 0.3634909987449646),<br>\n",
       "('radhika', 0.36309152841567993)]</td><td>[('comedy', 0.5931606292724609),<br>\n",
       "('drama', 0.5336683988571167),<br>\n",
       "('comedy-drama', 0.5062000751495361),<br>\n",
       "('thriller', 0.49741896986961365),<br>\n",
       "('farce', 0.4707985520362854),<br>\n",
       "('grant-dunne', 0.45909011363983154),<br>\n",
       "('action-drama', 0.4574933648109436),<br>\n",
       "('romp', 0.4551568627357483),<br>\n",
       "('romance', 0.43893399834632874),<br>\n",
       "('spy-thriller', 0.42537903785705566),<br>\n",
       "('worked-out', 0.4247702360153198),<br>\n",
       "('gem', 0.41165950894355774),<br>\n",
       "('romcom', 0.40875211358070374),<br>\n",
       "('`calendar', 0.40539270639419556),<br>\n",
       "('reformer', 0.4053284823894501),<br>\n",
       "('sharp-shooter', 0.3985632359981537),<br>\n",
       "('coming-of-age', 0.39505794644355774),<br>\n",
       "('fairhead', 0.39106929302215576),<br>\n",
       "('dramedy', 0.384566068649292),<br>\n",
       "(\"hitchcocks'\", 0.3830510973930359)]</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from IPython.display import HTML\n",
    "# pick a random word with a suitable number of occurences\n",
    "while True:\n",
    "    word = random.choice(word_models[0].wv.index2word)\n",
    "    if word_models[0].wv.vocab[word].count > 10:\n",
    "        break\n",
    "# or uncomment below line, to just pick a word from the relevant domain:\n",
    "word = 'comedy/drama'\n",
    "similars_per_model = [str(model.most_similar(word, topn=20)).replace('), ','),<br>\\n') for model in word_models]\n",
    "similar_table = (\"<table><tr><th>\" +\n",
    "    \"</th><th>\".join([str(model) for model in word_models]) + \n",
    "    \"</th></tr><tr><td>\" +\n",
    "    \"</td><td>\".join(similars_per_model) +\n",
    "    \"</td></tr></table>\")\n",
    "print(\"most similar words for '%s' (%d occurences)\" % (word, simple_models[0].wv.vocab[word].count))\n",
    "HTML(similar_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Do the DBOW words look meaningless? That's because the gensim DBOW model doesn't train word vectors – they remain at their random initialized values – unless you ask with the `dbow_words=1` initialization parameter. Concurrent word-training slows DBOW mode significantly, and offers little improvement (and sometimes a little worsening) of the error rate on this IMDB sentiment-prediction task. \n",
    "\n",
    "Words from DM models tend to show meaningfully similar words when there are many examples in the training data (as with 'plot' or 'actor'). (All DM modes inherently involve word vector training concurrent with doc vector training.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Are the word vectors from this dataset any good at analogies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# assuming something like\n",
    "# https://word2vec.googlecode.com/svn/trunk/questions-words.txt \n",
    "# is in local directory\n",
    "# note: this takes many minutes\n",
    "for model in word_models:\n",
    "    sections = model.accuracy('questions-words.txt')\n",
    "    correct, incorrect = len(sections[-1]['correct']), len(sections[-1]['incorrect'])\n",
    "    print('%s: %0.2f%% correct (%d of %d)' % (model, float(correct*100)/(correct+incorrect), correct, correct+incorrect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Even though this is a tiny, domain-specific dataset, it shows some meager capability on the general word analogies – at least for the DM/concat and DM/mean models which actually train word vectors. (The untrained random-initialized words of the DBOW model of course fail miserably.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Slop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "This cell left intentionally erroneous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To mix the Google dataset (if locally available) into the word tests..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "w2v_g100b = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "w2v_g100b.compact_name = 'w2v_g100b'\n",
    "word_models.append(w2v_g100b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To get copious logging output from above steps..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "rootLogger = logging.getLogger()\n",
    "rootLogger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To auto-reload python code while developing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
