{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# gensim doc2vec & IMDB sentiment dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "TODO: section on introduction & motivation\n",
    "\n",
    "TODO: prerequisites + dependencies (statsmodels, patsy, ?)\n",
    "\n",
    "### Requirements\n",
    "Following are the dependencies for this tutorial:\n",
    "    - testfixtures\n",
    "    - statsmodels\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Fetch and prep exactly as in Mikolov's go.sh shell script. (Note this cell tests for existence of required files, so steps won't repeat once the final summary file (`aclImdb/alldata-id.txt`) is available alongside this notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "files = ['train_pos', 'train_neg', 'test_pos', 'test_neg', 'train_unsup']\n",
    "\n",
    "with open('../data/imdb_sentiment/alldata-id.txt', 'w', encoding='utf-8') as out_sr:\n",
    "    i = 0\n",
    "    for file in files:\n",
    "        with open(os.path.join('../data/imdb_sentiment', f'{file}.txt'), 'r', encoding='utf-8') as in_sr:\n",
    "            for line in in_sr:\n",
    "                out_sr.write(f'_*{i} {line}')\n",
    "                i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The data is small enough to be read into memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 docs: 25000 train-sentiment, 25000 test-sentiment\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from collections import namedtuple\n",
    "\n",
    "SentimentDocument = namedtuple('SentimentDocument', 'words tags split sentiment')\n",
    "\n",
    "alldocs = []  # will hold all docs in original order\n",
    "with open('../data/imdb_sentiment/alldata-id.txt', encoding='utf-8') as alldata:\n",
    "    for line_no, line in enumerate(alldata):\n",
    "        tokens = gensim.utils.to_unicode(line).split()\n",
    "        words = tokens[1:]\n",
    "        tags = [line_no] # `tags = [tokens[0]]` would also work at extra memory cost\n",
    "        split = ['train','test','extra','extra'][line_no//25000]  # 25k train, 25k test, 25k extra\n",
    "        sentiment = [1.0, 0.0, 1.0, 0.0, None, None, None, None][line_no//12500] # [12.5K pos, 12.5K neg]*2 then unknown\n",
    "        alldocs.append(SentimentDocument(words, tags, split, sentiment))\n",
    "\n",
    "train_docs = [doc for doc in alldocs if doc.split == 'train']\n",
    "test_docs = [doc for doc in alldocs if doc.split == 'test']\n",
    "doc_list = alldocs[:]  # for reshuffling per pass\n",
    "\n",
    "print('%d docs: %d train-sentiment, %d test-sentiment' % (len(doc_list), len(train_docs), len(test_docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Set-up Doc2Vec Training & Evaluation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Approximating experiment of Le & Mikolov [\"Distributed Representations of Sentences and Documents\"](http://cs.stanford.edu/~quocle/paragraph_vector.pdf), also with guidance from Mikolov's [example go.sh](https://groups.google.com/d/msg/word2vec-toolkit/Q49FIrNOQRo/J6KG8mUj45sJ):\n",
    "\n",
    "`./word2vec -train ../alldata-id.txt -output vectors.txt -cbow 0 -size 100 -window 10 -negative 5 -hs 0 -sample 1e-4 -threads 40 -binary 0 -iter 20 -min-count 1 -sentence-vectors 1`\n",
    "\n",
    "Parameter choices below vary:\n",
    "\n",
    "* 100-dimensional vectors, as the 400d vectors of the paper don't seem to offer much benefit on this task\n",
    "* similarly, frequent word subsampling seems to decrease sentiment-prediction accuracy, so it's left out\n",
    "* `cbow=0` means skip-gram which is equivalent to the paper's 'PV-DBOW' mode, matched in gensim with `dm=0`\n",
    "* added to that DBOW model are two DM models, one which averages context vectors (`dm_mean`) and one which concatenates them (`dm_concat`, resulting in a much larger, slower, more data-hungry model)\n",
    "* a `min_count=2` saves quite a bit of model memory, discarding only words that appear in a single doc (and are thus no more expressive than the unique-to-each doc vectors themselves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)\n",
      "Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)\n",
      "Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "from collections import OrderedDict\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"this will be painfully slow otherwise\"\n",
    "\n",
    "simple_models = [\n",
    "    # PV-DM w/concatenation - window=5 (both sides) approximates paper's 10-word total window size\n",
    "    Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=5, hs=0, min_count=2, workers=cores, iter=1),\n",
    "    # PV-DBOW \n",
    "    Doc2Vec(dm=0, size=100, negative=5, hs=0, min_count=2, workers=cores, iter=1),\n",
    "    # PV-DM w/average\n",
    "    Doc2Vec(dm=1, dm_mean=1, size=100, window=10, negative=5, hs=0, min_count=2, workers=cores, iter=1),\n",
    "]\n",
    "\n",
    "for model in simple_models:\n",
    "    model.build_vocab(alldocs)\n",
    "    print(model)\n",
    "\n",
    "models_by_name = OrderedDict((str(model), model) for model in simple_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Following the paper, we also evaluate models in pairs. These wrappers return the concatenation of the vectors from each model. (Only the singular models are trained.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "models_by_name['dbow+dmm'] = ConcatenatedDoc2Vec([simple_models[1], simple_models[2]])\n",
    "models_by_name['dbow+dmc'] = ConcatenatedDoc2Vec([simple_models[1], simple_models[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Predictive Evaluation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Helper methods for evaluating error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from random import sample\n",
    "\n",
    "# for timing\n",
    "from contextlib import contextmanager\n",
    "from timeit import default_timer\n",
    "import time \n",
    "\n",
    "@contextmanager\n",
    "def elapsed_timer():\n",
    "    start = default_timer()\n",
    "    elapser = lambda: default_timer() - start\n",
    "    yield lambda: elapser()\n",
    "    end = default_timer()\n",
    "    elapser = lambda: end-start\n",
    "    \n",
    "def logistic_predictor_from_data(train_targets, train_regressors):\n",
    "    logit = sm.Logit(train_targets, train_regressors)\n",
    "    predictor = logit.fit(disp=0)\n",
    "    #print(predictor.summary())\n",
    "    return predictor\n",
    "\n",
    "def error_rate_for_model(test_model, train_set, test_set, infer=False, infer_steps=3, infer_alpha=0.1, infer_subsample=0.1):\n",
    "    \"\"\"Report error rate on test_doc sentiments, using supplied model and train_docs\"\"\"\n",
    "\n",
    "    train_targets, train_regressors = zip(*[(doc.sentiment, test_model.docvecs[doc.tags[0]]) for doc in train_set])\n",
    "    train_regressors = sm.add_constant(train_regressors)\n",
    "    predictor = logistic_predictor_from_data(train_targets, train_regressors)\n",
    "\n",
    "    test_data = test_set\n",
    "    if infer:\n",
    "        if infer_subsample < 1.0:\n",
    "            test_data = sample(test_data, int(infer_subsample * len(test_data)))\n",
    "        test_regressors = [test_model.infer_vector(doc.words, steps=infer_steps, alpha=infer_alpha) for doc in test_data]\n",
    "    else:\n",
    "        test_regressors = [test_model.docvecs[doc.tags[0]] for doc in test_docs]\n",
    "    test_regressors = sm.add_constant(test_regressors)\n",
    "    \n",
    "    # predict & evaluate\n",
    "    test_predictions = predictor.predict(test_regressors)\n",
    "    corrects = sum(np.rint(test_predictions) == [doc.sentiment for doc in test_data])\n",
    "    errors = len(test_predictions) - corrects\n",
    "    error_rate = float(errors) / len(test_predictions)\n",
    "    return (error_rate, errors, len(test_predictions), predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Bulk Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Using explicit multiple-pass, alpha-reduction approach as sketched in [gensim doc2vec blog post](http://radimrehurek.com/2014/12/doc2vec-tutorial/) – with added shuffling of corpus on each pass.\n",
    "\n",
    "Note that vector training is occurring on *all* documents of the dataset, which includes all TRAIN/TEST/DEV docs.\n",
    "\n",
    "Evaluation of each model's sentiment-predictive power is repeated after each pass, as an error rate (lower is better), to see the rates-of-relative-improvement. The base numbers reuse the TRAIN and TEST vectors stored in the models for the logistic regression, while the _inferred_ results use newly-inferred TEST vectors. \n",
    "\n",
    "(On a 4-core 2.6Ghz Intel Core i7, these 20 passes training and evaluating 3 main models takes about an hour.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "best_error = defaultdict(lambda :1.0)  # to selectively-print only best errors achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START 2017-04-01 18:01:58.502294\n",
      "*0.341360 : 1 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 40.3s 0.5s\n",
      "*0.344400 : 1 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)_inferred 40.3s 6.4s\n",
      "*0.145080 : 1 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 10.5s 0.5s\n",
      "*0.167600 : 1 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)_inferred 10.5s 2.4s\n",
      "*0.211400 : 1 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 16.2s 0.5s\n",
      " 0.212000 : 1 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)_inferred 16.2s 3.3s\n",
      "*0.138680 : 1 passes : dbow+dmm 0.0s 1.4s\n",
      "*0.154800 : 1 passes : dbow+dmm_inferred 0.0s 6.5s\n",
      "*0.145040 : 1 passes : dbow+dmc 0.0s 1.4s\n",
      "*0.184800 : 1 passes : dbow+dmc_inferred 0.0s 10.4s\n",
      "completed pass 1 at alpha 0.025000\n",
      "*0.301440 : 2 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 40.6s 0.5s\n",
      "*0.125080 : 2 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 10.7s 0.5s\n",
      "*0.191320 : 2 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 16.6s 0.5s\n",
      "*0.121920 : 2 passes : dbow+dmm 0.0s 1.5s\n",
      "*0.127520 : 2 passes : dbow+dmc 0.0s 1.5s\n",
      "completed pass 2 at alpha 0.023800\n",
      "*0.269440 : 3 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 40.8s 0.5s\n",
      "*0.116000 : 3 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 11.1s 0.5s\n",
      "*0.181680 : 3 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 16.9s 0.5s\n",
      "*0.113120 : 3 passes : dbow+dmm 0.0s 1.5s\n",
      "*0.116200 : 3 passes : dbow+dmc 0.0s 1.5s\n",
      "completed pass 3 at alpha 0.022600\n",
      "*0.246600 : 4 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 39.5s 0.9s\n",
      "*0.111720 : 4 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 11.2s 0.5s\n",
      "*0.173840 : 4 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 17.4s 0.5s\n",
      "*0.110440 : 4 passes : dbow+dmm 0.0s 1.5s\n",
      "*0.111360 : 4 passes : dbow+dmc 0.0s 1.6s\n",
      "completed pass 4 at alpha 0.021400\n",
      "*0.227960 : 5 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 39.8s 0.5s\n",
      "*0.219600 : 5 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)_inferred 39.8s 6.6s\n",
      "*0.108160 : 5 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 11.6s 0.5s\n",
      "*0.107600 : 5 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)_inferred 11.6s 2.7s\n",
      "*0.170160 : 5 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 17.1s 0.5s\n",
      " 0.214400 : 5 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)_inferred 17.1s 3.3s\n",
      "*0.107560 : 5 passes : dbow+dmm 0.0s 1.8s\n",
      "*0.122800 : 5 passes : dbow+dmm_inferred 0.0s 7.2s\n",
      "*0.108400 : 5 passes : dbow+dmc 0.0s 1.4s\n",
      "*0.117600 : 5 passes : dbow+dmc_inferred 0.0s 9.8s\n",
      "completed pass 5 at alpha 0.020200\n",
      "*0.216120 : 6 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 39.2s 0.5s\n",
      " 0.108240 : 6 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 11.2s 0.5s\n",
      "*0.165800 : 6 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 16.7s 0.5s\n",
      " 0.107920 : 6 passes : dbow+dmm 0.0s 1.5s\n",
      " 0.108600 : 6 passes : dbow+dmc 0.0s 1.4s\n",
      "completed pass 6 at alpha 0.019000\n",
      "*0.204920 : 7 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 37.2s 0.5s\n",
      "*0.105920 : 7 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 10.5s 0.6s\n",
      "*0.161920 : 7 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 16.2s 0.9s\n",
      "*0.105880 : 7 passes : dbow+dmm 0.0s 1.5s\n",
      "*0.105840 : 7 passes : dbow+dmc 0.0s 1.5s\n",
      "completed pass 7 at alpha 0.017800\n",
      "*0.196240 : 8 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 36.8s 0.5s\n",
      "*0.105280 : 8 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 10.4s 0.5s\n",
      "*0.160120 : 8 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 16.0s 0.5s\n",
      "*0.105080 : 8 passes : dbow+dmm 0.0s 1.4s\n",
      "*0.104720 : 8 passes : dbow+dmc 0.0s 1.5s\n",
      "completed pass 8 at alpha 0.016600\n",
      "*0.189680 : 9 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 36.5s 0.5s\n",
      "*0.103680 : 9 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 10.4s 0.5s\n",
      " 0.160160 : 9 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 16.1s 0.5s\n",
      "*0.103400 : 9 passes : dbow+dmm 0.0s 1.9s\n",
      "*0.103040 : 9 passes : dbow+dmc 0.0s 1.4s\n",
      "completed pass 9 at alpha 0.015400\n",
      "*0.187680 : 10 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 36.1s 0.5s\n",
      "*0.208000 : 10 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)_inferred 36.1s 6.3s\n",
      "*0.103400 : 10 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 10.4s 0.6s\n",
      " 0.112000 : 10 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)_inferred 10.4s 2.5s\n",
      "*0.158520 : 10 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 16.1s 0.5s\n",
      "*0.195200 : 10 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)_inferred 16.1s 3.4s\n",
      " 0.103720 : 10 passes : dbow+dmm 0.0s 1.5s\n",
      "*0.115200 : 10 passes : dbow+dmm_inferred 0.0s 6.4s\n",
      "*0.102600 : 10 passes : dbow+dmc 0.0s 1.5s\n",
      "*0.113200 : 10 passes : dbow+dmc_inferred 0.0s 10.0s\n",
      "completed pass 10 at alpha 0.014200\n",
      "*0.182480 : 11 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 35.8s 0.5s\n",
      "*0.102520 : 11 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 10.4s 0.6s\n",
      "*0.156280 : 11 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 16.2s 0.5s\n",
      "*0.103120 : 11 passes : dbow+dmm 0.0s 1.5s\n",
      " 0.102920 : 11 passes : dbow+dmc 0.0s 1.5s\n",
      "completed pass 11 at alpha 0.013000\n",
      "*0.178960 : 12 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 35.9s 0.5s\n",
      " 0.103120 : 12 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 10.5s 0.5s\n",
      "*0.155760 : 12 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 16.1s 0.5s\n",
      "*0.102760 : 12 passes : dbow+dmm 0.0s 1.4s\n",
      " 0.103760 : 12 passes : dbow+dmc 0.0s 1.4s\n",
      "completed pass 12 at alpha 0.011800\n",
      "*0.177400 : 13 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 35.5s 0.5s\n",
      "*0.102400 : 13 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 10.4s 0.9s\n",
      "*0.154400 : 13 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 16.0s 0.5s\n",
      " 0.102880 : 13 passes : dbow+dmm 0.0s 1.4s\n",
      " 0.104840 : 13 passes : dbow+dmc 0.0s 1.5s\n",
      "completed pass 13 at alpha 0.010600\n",
      "*0.175640 : 14 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 35.1s 0.5s\n",
      " 0.103360 : 14 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 10.4s 0.6s\n",
      "*0.153920 : 14 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 16.0s 0.5s\n",
      " 0.102920 : 14 passes : dbow+dmm 0.0s 1.6s\n",
      " 0.103880 : 14 passes : dbow+dmc 0.0s 1.4s\n",
      "completed pass 14 at alpha 0.009400\n",
      "*0.174560 : 15 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 35.0s 0.5s\n",
      "*0.186000 : 15 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)_inferred 35.0s 6.6s\n",
      " 0.103680 : 15 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 10.4s 0.5s\n",
      " 0.108800 : 15 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)_inferred 10.4s 2.5s\n",
      "*0.153000 : 15 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 16.2s 0.5s\n",
      "*0.169600 : 15 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)_inferred 16.2s 3.3s\n",
      " 0.102920 : 15 passes : dbow+dmm 0.0s 1.4s\n",
      "*0.105200 : 15 passes : dbow+dmm_inferred 0.0s 6.1s\n",
      " 0.103480 : 15 passes : dbow+dmc 0.0s 1.5s\n",
      "*0.100400 : 15 passes : dbow+dmc_inferred 0.0s 9.5s\n",
      "completed pass 15 at alpha 0.008200\n",
      "*0.173560 : 16 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 34.8s 0.5s\n",
      " 0.102960 : 16 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 10.4s 0.5s\n",
      "*0.152960 : 16 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 16.0s 0.9s\n",
      "*0.102440 : 16 passes : dbow+dmm 0.0s 1.5s\n",
      " 0.103400 : 16 passes : dbow+dmc 0.0s 1.4s\n",
      "completed pass 16 at alpha 0.007000\n",
      "*0.171400 : 17 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 34.6s 0.5s\n",
      " 0.102760 : 17 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 10.4s 0.5s\n",
      "*0.151760 : 17 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 16.0s 0.5s\n",
      " 0.102800 : 17 passes : dbow+dmm 0.0s 1.4s\n",
      " 0.103640 : 17 passes : dbow+dmc 0.0s 1.5s\n",
      "completed pass 17 at alpha 0.005800\n",
      "*0.170840 : 18 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 37.8s 0.5s\n",
      " 0.102680 : 18 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 11.8s 0.5s\n",
      "*0.151240 : 18 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 16.6s 0.5s\n",
      "*0.102400 : 18 passes : dbow+dmm 0.0s 2.0s\n",
      " 0.102920 : 18 passes : dbow+dmc 0.0s 1.4s\n",
      "completed pass 18 at alpha 0.004600\n",
      "*0.169960 : 19 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 36.3s 0.6s\n",
      " 0.102560 : 19 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 11.3s 0.5s\n",
      " 0.151320 : 19 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 17.3s 0.5s\n",
      " 0.102480 : 19 passes : dbow+dmm 0.0s 1.6s\n",
      " 0.103120 : 19 passes : dbow+dmc 0.0s 1.5s\n",
      "completed pass 19 at alpha 0.003400\n",
      " 0.170200 : 20 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 36.5s 0.5s\n",
      " 0.192800 : 20 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)_inferred 36.5s 6.2s\n",
      " 0.102640 : 20 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 11.3s 0.5s\n",
      "*0.098400 : 20 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)_inferred 11.3s 2.6s\n",
      "*0.151080 : 20 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 17.1s 0.6s\n",
      " 0.185200 : 20 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)_inferred 17.1s 3.9s\n",
      "*0.102320 : 20 passes : dbow+dmm 0.0s 1.6s\n",
      " 0.108000 : 20 passes : dbow+dmm_inferred 0.0s 6.7s\n",
      " 0.103760 : 20 passes : dbow+dmc 0.0s 1.6s\n",
      " 0.106000 : 20 passes : dbow+dmc_inferred 0.0s 9.6s\n",
      "completed pass 20 at alpha 0.002200\n",
      "END 2017-04-01 18:27:25.898038\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "import datetime\n",
    "\n",
    "alpha, min_alpha, passes = (0.025, 0.001, 20)\n",
    "alpha_delta = (alpha - min_alpha) / passes\n",
    "\n",
    "print(\"START %s\" % datetime.datetime.now())\n",
    "\n",
    "for epoch in range(passes):\n",
    "    shuffle(doc_list)  # shuffling gets best results\n",
    "    \n",
    "    for name, train_model in models_by_name.items():\n",
    "        # train\n",
    "        duration = 'na'\n",
    "        train_model.alpha, train_model.min_alpha = alpha, alpha\n",
    "        with elapsed_timer() as elapsed:\n",
    "            train_model.train(doc_list)\n",
    "            duration = '%.1f' % elapsed()\n",
    "            \n",
    "        # evaluate\n",
    "        eval_duration = ''\n",
    "        with elapsed_timer() as eval_elapsed:\n",
    "            err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs, test_docs)\n",
    "        eval_duration = '%.1f' % eval_elapsed()\n",
    "        best_indicator = ' '\n",
    "        if err <= best_error[name]:\n",
    "            best_error[name] = err\n",
    "            best_indicator = '*' \n",
    "        print(\"%s%f : %i passes : %s %ss %ss\" % (best_indicator, err, epoch + 1, name, duration, eval_duration))\n",
    "\n",
    "        if ((epoch + 1) % 5) == 0 or epoch == 0:\n",
    "            eval_duration = ''\n",
    "            with elapsed_timer() as eval_elapsed:\n",
    "                infer_err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs, test_docs, infer=True)\n",
    "            eval_duration = '%.1f' % eval_elapsed()\n",
    "            best_indicator = ' '\n",
    "            if infer_err < best_error[name + '_inferred']:\n",
    "                best_error[name + '_inferred'] = infer_err\n",
    "                best_indicator = '*'\n",
    "            print(\"%s%f : %i passes : %s %ss %ss\" % (best_indicator, infer_err, epoch + 1, name + '_inferred', duration, eval_duration))\n",
    "\n",
    "    print('completed pass %i at alpha %f' % (epoch + 1, alpha))\n",
    "    alpha -= alpha_delta\n",
    "    \n",
    "print(\"END %s\" % str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Achieved Sentiment-Prediction Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.098400 Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)_inferred\n",
      "0.100400 dbow+dmc_inferred\n",
      "0.102320 dbow+dmm\n",
      "0.102400 Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)\n",
      "0.102600 dbow+dmc\n",
      "0.105200 dbow+dmm_inferred\n",
      "0.151080 Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)\n",
      "0.169600 Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)_inferred\n",
      "0.169960 Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)\n",
      "0.186000 Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)_inferred\n"
     ]
    }
   ],
   "source": [
    "# print best error rates achieved\n",
    "for rate, name in sorted((rate, name) for name, rate in best_error.items()):\n",
    "    print(\"%f %s\" % (rate, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In my testing, unlike the paper's report, DBOW performs best. Concatenating vectors from different models only offers a small predictive improvement. The best results I've seen are still just under 10% error rate, still a ways from the paper's 7.42%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Examining Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Are inferred vectors close to the precalculated ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for doc 29980...\n",
      "Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8):\n",
      " [(29980, 0.7381277084350586), (54217, 0.43917858600616455), (89949, 0.43274450302124023)]\n",
      "Doc2Vec(dbow,d100,n5,mc2,s0.001,t8):\n",
      " [(29980, 0.9458428621292114), (64037, 0.5815826058387756), (54200, 0.5800715684890747)]\n",
      "Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8):\n",
      " [(29980, 0.8344322443008423), (74433, 0.6969673037528992), (94671, 0.6919772028923035)]\n"
     ]
    }
   ],
   "source": [
    "doc_id = np.random.randint(simple_models[0].docvecs.count)  # pick random doc; re-run cell for more examples\n",
    "print('for doc %d...' % doc_id)\n",
    "for model in simple_models:\n",
    "    inferred_docvec = model.infer_vector(alldocs[doc_id].words)\n",
    "    print('%s:\\n %s' % (model, model.docvecs.most_similar([inferred_docvec], topn=3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "(Yes, here the stored vector from 20 epochs of training is usually one of the closest to a freshly-inferred vector for the same words. Note the defaults for inference are very abbreviated – just 3 steps starting at a high alpha – and likely need tuning for other applications.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Do close documents seem more related than distant ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET (2387): «i have a six month old baby at home and time to time she fights sleep really bad . one morning she was having a particular difficult time getting to sleep when the doodle bops theme song came on t . v . she stopped crying almost instantly , and for the rest of the show was content . i sat her in her bouncy seat and watched her kick her legs , swing her arms , and actually laugh at this show . the kept her entertained and happy the entire time . i also got a video of them so that at times when my little one is flustered i have something to calm her . granted , late at night if she awakes with colic to fuss the doodle bops are not her cup of tea , but they sure do come in handy when i need a little time to do housework , etc . the biggest surprise about the doodle bops is that my child doesn't even like watching t . v . she'd rather be in the floor playing with a toy or with our small toy poodle than watch t . v . yet , the doodle bops have totally captured her attention . i don't know if she will continue to like them in the future but for now she's attached .»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dbow,d100,n5,mc2,s0.001,t8):\n",
      "\n",
      "MOST (47584, 0.5678682327270508): «tyra & the rest of the modeling world needs to know that real women like myself and my daughter don't care to see all the ridiculous modeling to sell something . weird locations , too much makeup & too much skin is not necessary . sex does not always sell when you are selling to women . the same goes for the horse stomping runway walk that looks unnatural . people come in all shapes & sizes & they need to have that on the show . my daughter has a 36 \" inseam , is tall & slender & a size 5 , i am more average at a size 12 . we would like to see both- i can not picture how something would look on me when a size 2 is wearing it , it will not fit the same way on me . i do not buy magazines anymore because they are one sided on this matter . we would really love the show to consider women of all sizes . thank you .»\n",
      "\n",
      "MEDIAN (94912, 0.23660564422607422): «if this movie provides any insight into the chinese psyche , i'm worried . first off this is a kids movie with a cute little alien creature that a boy befriends . except the boy doesn't befriend him . he tries use him at first to get stuff , like good grades , and when the alien fails the alien is tortured , beaten , and thrown in the trash . for some reason , the alien returns to the boys home where he is further beaten and hit and thrown around . the alien decides to stay again . i am going to reveal the ending . . . the movie ends with the father being killed at an accident at work and the cute fuzzy alien thing sacrificing himself to resurrect the father . the only sense of the story i can make is that the creature was so despondent at being left in such a cruel miserable country he decided to kill himself by resurrecting the father . he resurrected the father because he wanted to punish him for being such a jerk by making him live longer at his sucky life as a poverty stricken coolie . and don't worry , the boy gets beaten too . great kids movie . want to know why china is one of the worst offenders when it comes to animal cruelty ? here's one reason why . now i am no huge fan of disney/pixar/dreamworks , but at least the messages in their movies have a beautiful sentiment behind them . you know , friendship is important , love is important , etc etc . the message in this movie seems to be you can treat non-human living beings as horrible as you want , use them , dispose of them , and you will still be their master . if this is what passes for children's entertainment in china , then that is one messed up country . i should also mention , this movie is painfully unfunny . most american kids movies have a lot of clever things to keep adults entertained along with the young ones . like , for example , the use of led zeppelin's immigrant song in shriek iii . there is none of that in this movie . it is extremely dumbed down . but it won't work for kids either , as it can be upsetting to sensitive children . so , unless you are interested in a sociological examination of what constitutes a kid's movie from china , stay far away from this one . shaolin soccer isn't too bad though . i don't know where all the positive reviews came from either . i think it must be astroturfing by people who work for the studio .»\n",
      "\n",
      "LEAST (41425, -0.1130366325378418): «this movie is poorly written , hard-to-follow , and features bad performances and dialog from leads jason patric and jennifer jason leigh . the premise , believable but weak ( undercover narcotics agent succumbs to the drug underworld ) deserved better than this lili fini zanuck flop . the competent supporting cast ( sam elliott , william sadler , others ) was not enough to save this film . in addition , this movie also contains the absolute worst \" love \" scene in cinema . moreover , the soundtrack is vastly overrated ; specifically the revolting , sappy-without-substance \" tears in heaven \" by the otherwise legendary eric clapton . \" rush \" is wholly unenjoyable from beginning to end . 2 of 10»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "doc_id = np.random.randint(simple_models[0].docvecs.count)  # pick random doc, re-run cell for more examples\n",
    "model = random.choice(simple_models)  # and a random model\n",
    "sims = model.docvecs.most_similar(doc_id, topn=model.docvecs.count)  # get *all* similar documents\n",
    "print(u'TARGET (%d): «%s»\\n' % (doc_id, ' '.join(alldocs[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(alldocs[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "(Somewhat, in terms of reviewer tone, movie genre, etc... the MOST cosine-similar docs usually seem more like the TARGET than the MEDIAN or LEAST.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Do the word vectors show useful similarities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_models = simple_models[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most similar words for 'vampire' (1638 occurences)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)</th><th>Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)</th><th>Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)</th></tr><tr><td>[('zombie', 0.7549912929534912),<br>\n",
       "('werewolf', 0.6992056369781494),<br>\n",
       "('superhero', 0.6700093746185303),<br>\n",
       "('cannibal', 0.6675169467926025),<br>\n",
       "('slasher', 0.6568142771720886),<br>\n",
       "('gangster', 0.6508501768112183),<br>\n",
       "('ghost', 0.6486688852310181),<br>\n",
       "('monster', 0.6469652652740479),<br>\n",
       "('chick', 0.6054737567901611),<br>\n",
       "('teen', 0.6036348342895508),<br>\n",
       "('horror', 0.596318781375885),<br>\n",
       "('cyborg', 0.5920171737670898),<br>\n",
       "('blaxploitation', 0.5856066942214966),<br>\n",
       "('biker', 0.5802656412124634),<br>\n",
       "('ninja', 0.578728199005127),<br>\n",
       "('gangsta', 0.578520655632019),<br>\n",
       "('psycho', 0.5761145949363708),<br>\n",
       "('shark', 0.5752491354942322),<br>\n",
       "('dinosaur', 0.573762834072113),<br>\n",
       "('porno', 0.5736423134803772)]</td><td>[('hoast', 0.4167806804180145),<br>\n",
       "('matchbox', 0.4157073497772217),<br>\n",
       "('meridian', 0.40402379631996155),<br>\n",
       "('frawley', 0.40150338411331177),<br>\n",
       "('terrible-looking', 0.38961461186408997),<br>\n",
       "('flordia', 0.3872451186180115),<br>\n",
       "('forgoes', 0.3839855492115021),<br>\n",
       "('giulia', 0.3836101293563843),<br>\n",
       "('picket', 0.38231295347213745),<br>\n",
       "(\"liv's\", 0.3742333948612213),<br>\n",
       "('foot-long', 0.37398484349250793),<br>\n",
       "('wispy', 0.3702859878540039),<br>\n",
       "('pääkkönen', 0.36951944231987),<br>\n",
       "(\"companion'\", 0.3691626191139221),<br>\n",
       "(\"have'nt\", 0.3662981390953064),<br>\n",
       "('edita', 0.36490097641944885),<br>\n",
       "('formatted', 0.3641866445541382),<br>\n",
       "('vibration', 0.36348310112953186),<br>\n",
       "('carré', 0.36337175965309143),<br>\n",
       "('reinsertion', 0.363328218460083)]</td><td>[('werewolf', 0.6782216429710388),<br>\n",
       "('zombie', 0.6059867143630981),<br>\n",
       "('vampires', 0.5641380548477173),<br>\n",
       "('monster', 0.5572120547294617),<br>\n",
       "('mummy', 0.5348901748657227),<br>\n",
       "('horror', 0.5325940251350403),<br>\n",
       "('chick', 0.5273053050041199),<br>\n",
       "('dracula', 0.5144455432891846),<br>\n",
       "('waldemar', 0.5058416128158569),<br>\n",
       "('slasher', 0.5019328594207764),<br>\n",
       "('sorcerer', 0.49730193614959717),<br>\n",
       "('predator', 0.49664396047592163),<br>\n",
       "('yeti', 0.48820286989212036),<br>\n",
       "('slayer', 0.48651444911956787),<br>\n",
       "('snake', 0.48307961225509644),<br>\n",
       "('frankenstein', 0.48196864128112793),<br>\n",
       "('sorceress', 0.47515150904655457),<br>\n",
       "('bathory', 0.4714050889015198),<br>\n",
       "('creature', 0.4713159203529358),<br>\n",
       "('alien', 0.4614649713039398)]</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from IPython.display import HTML\n",
    "# pick a random word with a suitable number of occurences\n",
    "while True:\n",
    "    word = random.choice(word_models[0].wv.index2word)\n",
    "    if word_models[0].wv.vocab[word].count > 10:\n",
    "        break\n",
    "# or uncomment below line, to just pick a word from the relevant domain:\n",
    "#word = 'comedy/drama'\n",
    "similars_per_model = [str(model.most_similar(word, topn=20)).replace('), ','),<br>\\n') for model in word_models]\n",
    "similar_table = (\"<table><tr><th>\" +\n",
    "    \"</th><th>\".join([str(model) for model in word_models]) + \n",
    "    \"</th></tr><tr><td>\" +\n",
    "    \"</td><td>\".join(similars_per_model) +\n",
    "    \"</td></tr></table>\")\n",
    "print(\"most similar words for '%s' (%d occurences)\" % (word, simple_models[0].wv.vocab[word].count))\n",
    "HTML(similar_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Do the DBOW words look meaningless? That's because the gensim DBOW model doesn't train word vectors – they remain at their random initialized values – unless you ask with the `dbow_words=1` initialization parameter. Concurrent word-training slows DBOW mode significantly, and offers little improvement (and sometimes a little worsening) of the error rate on this IMDB sentiment-prediction task. \n",
    "\n",
    "Words from DM models tend to show meaningfully similar words when there are many examples in the training data (as with 'plot' or 'actor'). (All DM modes inherently involve word vector training concurrent with doc vector training.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Are the word vectors from this dataset any good at analogies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assuming something like\n",
    "# https://word2vec.googlecode.com/svn/trunk/questions-words.txt \n",
    "# is in local directory\n",
    "# note: this takes many minutes\n",
    "for model in word_models:\n",
    "    sections = model.accuracy('questions-words.txt')\n",
    "    correct, incorrect = len(sections[-1]['correct']), len(sections[-1]['incorrect'])\n",
    "    print('%s: %0.2f%% correct (%d of %d)' % (model, float(correct*100)/(correct+incorrect), correct, correct+incorrect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Even though this is a tiny, domain-specific dataset, it shows some meager capability on the general word analogies – at least for the DM/concat and DM/mean models which actually train word vectors. (The untrained random-initialized words of the DBOW model of course fail miserably.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Slop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "This cell left intentionally erroneous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To mix the Google dataset (if locally available) into the word tests..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "w2v_g100b = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "w2v_g100b.compact_name = 'w2v_g100b'\n",
    "word_models.append(w2v_g100b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To get copious logging output from above steps..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "rootLogger = logging.getLogger()\n",
    "rootLogger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To auto-reload python code while developing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
